export interface LectionHeader {
  id: string;
  title: string;
  order: number;
  content: string;
}

export interface LectionTopic {
  id: string;
  title: string;
  headers: LectionHeader[];
}

export const lectionTopics: LectionTopic[] = [
  {
    id: 'topic-1',
    title: 'Тема 1: Введение в анализ данных. Жизненный цикл данных',
    headers: [
      {
        id: 'topic-1-header-1',
        title: 'Что такое данные и их типы (ключевое для всего курса)',
        order: 1,
        content: 'Данные — это факты, наблюдения или измерения, представленные в формализованном виде.\nТипы данных по структуре:\nСтруктурированные: Таблицы (SQL, Excel). Имеют четкую схему: строки (наблюдения) и столбцы (признаки). Пример: данные о транзакциях.\nПолуструктурированные: Имеют нечеткую или гибкую схему (JSON, XML, HTML). Пример: лог-файлы, ответы API.\nНеструктурированные: Не имеют предопределенной табличной формы. Текст, изображения, аудио, видео. Анализ требует специфических методов (NLP, Computer Vision).',
      },
      {
        id: 'topic-1-header-2',
        title: 'Уровни аналитики (по возрастанию сложности и ценности)',
        order: 2,
        content: 'Descriptive Analytics (Что произошло?):\nСуть: Описание исторических данных. Агрегация и визуализация.\nИнструменты: SQL, сводные таблицы, дашборды (Tableau, Power BI).\nПример: «Вчера продажи в регионе А выросли на 15%».\nDiagnostic Analytics (Почему это произошло?):\nСуть: Поиск причинно-следственных связей, «копание» в данных.\nМетоды: Детализация (drill-down), корреляционный анализ, выявление аномалий.\nПример: «Рост продаж в регионе А связан с запуском рекламной кампании Х».\nPredictive Analytics (Что произойдет?):\nСуть: Прогнозирование будущих событий на основе исторических паттернов.\nМетоды: Регрессия, машинное обучение, временные ряды.\nПример: «На следующей неделе прогнозируется падение продаж товара Y на 7%».\nPrescriptive Analytics (Как сделать, чтобы...?):\nСуть: Рекомендация оптимальных действий для достижения цели.\nМетоды: Оптимизация, A/B-тестирование, симуляции.\nПример: «Чтобы увеличить прибыль на 10%, рекомендуем снизить цену на товар Z на 5% и увеличить бюджет на канал В».',
      },
      {
        id: 'topic-1-header-3',
        title: 'Детальный разбор CRISP-DM (с акцентом на практику)',
        order: 3,
        content: 'Каждый этап имеет свои ключевые выходные артефакты.\nBusiness Understanding:\nКлючевой вопрос: Какую бизнес-проблему мы решаем?\nАртефакты: Устное определение цели (Goal), критерии успеха (Success Criteria), план проекта.\nData Understanding:\nКлючевые действия: Сбор данных из всех доступных источников (базы данных, CSV, API). Первичный осмотр (Initial Data Exploration): df.head(), df.info(), df.describe(). Поиск явных проблем.\nАртефакты: Описание данных (Data Dictionary), список проблем качества.\nData Preparation (самый длинный этап):\nОсновные задачи:\nОбъединение данных (Merging/Joining): Сборка финального датасета из разных таблиц.\nОчистка (Cleaning): Работа с пропусками, дубликатами, неконсистентными записями (например, «М» и «Муж.»).\nПреобразование (Transformation): Создание новых признаков (Feature Engineering), нормализация, категоризация (binning).\nАртефакт: Чистый, готовый к анализу датасет.\nModeling:\nВыбор алгоритма зависит от типа задачи (классификация, регрессия, кластеризация).\nРазделение данных на обучающую (train) и тестовую (test) выборки до обучения для адекватной оценки.\nEvaluation:\nОценка не на обучающих данных, а на тестовых (hold-out set)!\nПроверка против бизнес-целей: Даже хорошая точность модели (Accuracy) может быть бесполезна, если она не решает изначальную проблему.\nРешение: Запустить ли модель в работу или вернуться на предыдущие этапы.\nDeployment:\nНе просто модель, а рабочий процесс: Модель должна быть обернута в API, интегрирована в приложение или автоматизированный отчет.\nМониторинг: Дрейф данных (Data Drift) — со временем реальные данные могут меняться, и модель будет деградировать. Нужен план ее переобучения.\n\nВывод по теме 1: Анализ данных — это структурированный, итеративный процесс преобразования сырых данных в знания и решения. Его суть заключается не в слепом применении инструментов, а в решении конкретной бизнес- или исследовательской проблемы. Модель CRISP-DM служит универсальной картой этого процесса, подчеркивая, что понимание задачи и подготовка данных являются критически важными этапами, на которые приходится львиная доля времени и усилий. Успешный анализ всегда начинается с правильного вопроса.',
      },
    ],
  },
  {
    id: 'topic-2',
    title: 'Тема 2: Основы Python для анализа',
    headers: [
      {
        id: 'topic-2-header-1',
        title: 'Философия Python для анализа: «Batteries included, but specialized tools rule»',
        order: 1,
        content: 'В Python много встроенного, но для анализа мы используем специализированные «тяжелые» библиотеки (pandas, numpy), которые написанны на C/C++ для скорости.',
      },
      {
        id: 'topic-2-header-2',
        title: 'Jupyter Notebook/Lab — ваша цифровая лаборатория',
        order: 2,
        content: 'Ячейки (Cells) и их типы:\nCode: Исполняемый код на Python.\nMarkdown: Текстовые пояснения, заголовки, формулы (LaTeX). Критически важно для документирования хода мысли!\nСостояние ядра (Kernel): Хранит в памяти все переменные после выполнения ячеек. Позволяет работать интерактивно. Проблема: Можно запутаться в состоянии. Решение — периодически перезапускать ядро (Kernel -> Restart & Run All).\nМагия (Magic commands): Команды, начинающиеся с % или %%. Например:\n%timeit my_function() — замер времени выполнения.\n%matplotlib inline — отображение графиков прямо в ноутбуке.\n%%writefile script.py — запись ячейки в файл.',
      },
      {
        id: 'topic-2-header-3',
        title: 'Критически важные концепции Python для аналитика',
        order: 3,
        content: 'Динамическая типизация: Тип переменной (int, str, list) определяется в момент присвоения значения, а не при объявлении.\nСсылочная модель: Переменная — это «имя» (ссылка) на объект в памяти.\npython\na = [1, 2, 3]\nb = a  # b теперь указывает на ТОТ ЖЕ список, что и a\nb.append(4)\nprint(a)  # [1, 2, 3, 4] !!! Изменение по ссылке\nЧтобы создать копию, нужно явно использовать .copy() или list(a).\nИмпорт библиотек и алиасы (соглашения):\npython\nimport numpy as np          # Стандартный алиас для NumPy\nimport pandas as pd         # Стандартный алиас для Pandas\nimport matplotlib.pyplot as plt # Стандартный алиас для Pyplot\nЭто экономит время и делает код общепринятым.',
      },
      {
        id: 'topic-2-header-4',
        title: 'Структура типичного аналитического проекта',
        order: 4,
        content: 'text\nmy_analysis_project/\n├── data/                 # Папка с данными (сырыми и обработанными)\n│   ├── raw/             # Исходные, неприкасаемые данные\n│   └── processed/       # Очищенные данные\n├── notebooks/           # Jupyter-ноутбуки с исследованием\n│   └── 01_eda.ipynb\n├── src/                 # Исходный код Python-модулей (функции, классы)\n│   └── data_preprocessing.py\n├── reports/             # Готовые отчеты, презентации, дашборды\n├── requirements.txt     # Список зависимостей (библиотек)\n└── README.md            # Описание проекта\n\nВывод по теме 2: Python стал главным языком анализа данных благодаря уникальному сочетанию простоты, читаемости и мощнейшей экосистемы специализированных библиотек. Интерактивная среда Jupyter Notebook является идеальным инструментом для исследований, позволяя смешивать код, визуализации и текстовые пояснения. Ключ к профессиональной работе — умение организовывать проекты и управлять зависимостями через виртуальные окружения, что обеспечивает воспроизводимость и стабильность результатов.',
      },
    ],
  },
  {
    id: 'topic-3',
    title: 'Тема 3: Типы данных и коллекции',
    headers: [
      {
        id: 'topic-3-header-1',
        title: 'Глубже в неизменяемость (Immutability)',
        order: 1,
        content: 'Зачем это нужно? Безопасность (защита от случайного изменения), возможность использовать объект как ключ словаря, хешируемость.\nПример: tuple идеален для хранения координат (x, y) или записи о человеке (id, name, birthday), которую логически менять не должны.\nfrozenset: Неизменяемая версия set. Можно использовать как ключ в словаре.',
      },
      {
        id: 'topic-3-header-2',
        title: 'Списки (list) — ваш основной «рабочий конь»',
        order: 2,
        content: 'Методы, которые меняют список на месте (in-place):\n.append(x) — добавить в конец (O(1)).\n.extend(iterable) — добавить элементы итерируемого объекта (как +, но эффективнее).\n.insert(i, x) — вставить на позицию i (O(n), может быть медленно).\n.remove(x) — удалить первый элемент со значением x.\n.pop([i]) — удалить и вернуть элемент по индексу (по умолчанию последний).\n.sort() — сортировка на месте. Ключевой аргумент: key=func (например, key=len для сортировки по длине строк).\n.reverse() — развернуть список на месте.\nМетоды, создающие новый объект:\nsorted(list) — возвращает новый отсортированный список, оригинал не трогает.\nreversed(list) — возвращает итератор (часто оборачивают в list()).\nГенераторы списков (List Comprehension): Элегантный и быстрый способ создания списков.\npython\n# Вместо этого:\nsquares = []\nfor x in range(10):\n    squares.append(x**2)\n# Пишем так:\nsquares = [x**2 for x in range(10)]\n# С условием:\neven_squares = [x**2 for x in range(10) if x % 2 == 0]',
      },
      {
        id: 'topic-3-header-3',
        title: 'Словари (dict) — основа для быстрого поиска и агрегации',
        order: 3,
        content: 'Ключом может быть только хешируемый (hashable) объект (неизменяемый: int, float, str, tuple).\nМетоды для безопасной работы:\n.get(key[, default]) — вернуть значение по ключу, если ключа нет — вернуть default (по умолчанию None). Не вызывает ошибку KeyError.\n.setdefault(key[, default]) — если ключ есть, вернуть его значение. Если нет — вставить ключ со значением default и вернуть default.\n.update(other_dict) — обновить словарь парами из другого словаря.\nСловари для агрегации (паттерн):\npython\n# Подсчет частоты элементов (слов в тексте, городов в списке)\ncounts = {}\nfor item in item_list:\n    counts[item] = counts.get(item, 0) + 1',
      },
      {
        id: 'topic-3-header-4',
        title: 'Множества (set) — уникальность и теория множеств',
        order: 4,
        content: 'Основные операции:\na | b (объединение) — элементы, которые в a, или в b, или в обоих.\na & b (пересечение) — элементы, которые и в a, и в b.\na - b (разность) — элементы, которые в a, но не в b.\na ^ b (симметрическая разность) — элементы, которые в a или в b, но не в обоих.\nПрактическое применение: Быстрая проверка на вхождение (x in set — O(1)), удаление дубликатов из списка (list(set(my_list))), нахождение общих элементов двух списков.\n\nВывод по теме 3: Понимание встроенных структур данных Python — фундамент для эффективной работы. Списки (list) и словари (dict) выступают основными "рабочими лошадками" для хранения и манипуляций данными в чистом Python, в то время как кортежи (tuple) обеспечивают безопасность неизменяемости, а множества (set) — уникальность и быстрые логические операции. Глубокое понимание их свойств (изменяемость, методы, принципы работы) позволяет писать не только корректный, но и производительный и понятный код.',
      },
    ],
  },
  {
    id: 'topic-4',
    title: 'Тема 4: NumPy',
    headers: [
      {
        id: 'topic-4-header-1',
        title: 'Массив (ndarray) — фундаментальная структура',
        order: 1,
        content: 'Однородность (Homogeneity): Все элементы одного типа (dtype). Это позволяет хранить данные плотно в памяти и эффективно применять операции.\ndtype — мощный инструмент управления памятью и точностью:\nnp.int8, np.int16, np.int32, np.int64 — целые числа разной емкости.\nnp.float16, np.float32, np.float64 — числа с плавающей точкой разной точности.\nnp.bool_ — булевы значения.\nnp.object_ — массив Python-объектов (теряет многие преимущества NumPy, используется редко).\nМожно указывать при создании: np.array([1,2,3], dtype=np.float32).',
      },
      {
        id: 'topic-4-header-2',
        title: 'Индексация: базовая, булева (маски) и "причудливая" (fancy)',
        order: 2,
        content: 'Булева индексация (Boolean Indexing) — один из самых мощных инструментов:\npython\narr = np.array([1, 2, 3, 4, 5])\nmask = arr > 2  # [False, False, True, True, True]\nfiltered = arr[mask]  # [3, 4, 5]\n# Или в одну строку:\narr[arr % 2 == 0]  # Все четные: [2, 4]\n"Причудливая" индексация (Fancy Indexing): Передача списка/массива индексов.\npython\narr = np.array([10, 20, 30, 40, 50])\nindices = [1, 3, 4]\narr[indices]  # [20, 40, 50]',
      },
      {
        id: 'topic-4-header-3',
        title: 'Операции и оси (axis) — ключ к пониманию агрегации',
        order: 3,
        content: 'Ось (axis) — это измерение массива. Для 2D массива:\naxis=0 — ось строк (вертикальная, "вниз по столбцам").\naxis=1 — ось столбцов (горизонтальная, "вдоль строк").\nАгрегация по оси:\npython\narr_2d = np.array([[1, 2, 3],\n                   [4, 5, 6]])\nnp.sum(arr_2d, axis=0)  # Сумма по столбцам: [5, 7, 9] (сложили две строки)\nnp.mean(arr_2d, axis=1) # Среднее по строкам: [2., 5.] (усреднили элементы в каждой строке)',
      },
      {
        id: 'topic-4-header-4',
        title: 'Трансляция (Broadcasting) — правила в деталях',
        order: 4,
        content: 'Позволяет выполнять операции между массивами разной формы без их явного копирования.\nПравила (по шагам):\nСравнить формы массивов справа налево.\nИзмерения считаются совместимыми, если:\nОни равны, или\nОдно из них равно 1.\nМассив с меньшей размерностью мысленно "растягивается" по измерениям размера 1.\nЕсли размерности не совместимы — ошибка.\nПримеры:\n(3, 4) + (4,) -> (3,4) + (1,4) -> (3,4) + (3,4) ✔\n(2, 3) + (3,) -> (2,3) + (1,3) -> (2,3) + (2,3) ✔\n(2, 3) + (2,) -> (2,3) + (1,2) -> (2,3) + (2,2) ❌ Ошибка! Последние размеры 3 и 2 не равны и ни одна не равна 1.\nПрактический пример:\npython\n# Вычитание среднего по столбцам из каждой строки (стандартизация)\ndata = np.array([[1, 2, 3],\n                 [4, 5, 6]])\ncolumn_means = data.mean(axis=0)  # Форма (3,)\ncentered_data = data - column_means  # Broadcasting: (2,3) - (3,) -> работает!',
      },
      {
        id: 'topic-4-header-5',
        title: 'Линейная алгебра "из коробки"',
        order: 5,
        content: 'np.dot(a, b) или a @ b — матричное умножение.\nnp.linalg.inv(a) — обратная матрица.\nnp.linalg.eig(a) — собственные значения и векторы.',
      },
      {
        id: 'topic-4-header-6',
        title: 'Производительность: Векторизация против циклов',
        order: 6,
        content: 'Векторизованные операции — это операции, выполняемые над целыми массивами с помощью скомпилированного C-кода. Всегда стремитесь к ним.\nИзбегайте явных циклов for по элементам массивов NumPy. Они сводят на нет все преимущества библиотеки.\n\nВывод по теме 4: NumPy решает ключевую проблему Python для численных расчётов — производительность — через введение однородного многомерного массива ndarray. Векторизованные операции и трансляция (broadcasting) позволяют заменять неэффективные циклы на быстрые вычисления на уровне скомпилированного C-кода. NumPy не просто ускоряет вычисления, он задаёт идеологию работы с числовыми данными в экосистеме Python, выступая фундаментом для всех высокоуровневых библиотек, таких как Pandas и Scikit-learn.',
      },
    ],
  },
  {
    id: 'topic-5',
    title: 'Тема 5: Работа с табличными данными в Pandas',
    headers: [
      {
        id: 'topic-5-header-1',
        title: 'Философия и основа Pandas',
        order: 1,
        content: 'Pandas — это высокоуровневая библиотека для работы с размеченными (labeled) и реляционными (табличными) данными. Построена на основе NumPy.\nКлючевая идея: Предоставить интуитивно понятные структуры данных и операции для работы с таблицами, подобные тем, что есть в SQL или Excel, но с производительностью и мощью Python.',
      },
      {
        id: 'topic-5-header-2',
        title: 'Две ключевые структуры данных',
        order: 2,
        content: 'Series: Упрощённо — "умный" одномерный массив с индексом (labels). Это столбец таблицы с его собственными метками строк.\nindex — метки строк (не только целые числа 0,1,2..., но и строки, даты).\nvalues — массив NumPy с данными.\ndtype — тип данных в столбце.\nDataFrame: Основная структура. Двумерная таблица с индексом строк и метками столбцов.\nЭто словарь из Series (столбцов), где ключ — название столбца.\nОснова для 90% работы: Все операции очистки, анализа, агрегации происходят с DataFrame.',
      },
      {
        id: 'topic-5-header-3',
        title: 'Создание DataFrame: основные способы',
        order: 3,
        content: 'Из словаря списков/массивов (ключи → названия столбцов).\nИз списка словарей (каждый словарь → строка).\nНаиболее важный: Чтение из внешних файлов:\npd.read_csv() — из CSV (основной формат).\npd.read_excel() — из Excel.\npd.read_sql() — из базы данных SQL.\nКлючевые параметры: sep, encoding, parse_dates, index_col.',
      },
      {
        id: 'topic-5-header-4',
        title: 'Инспекция и базовое исследование данных',
        order: 4,
        content: 'Форма и размер: .shape, .size.\nОбщая информация: .info() — типы данных, количество не-NULL значений в каждом столбце. Первый обязательный вызов.\nПросмотр данных: .head(), .tail(), .sample().\nОсновные статистики: .describe() — считает статистики только для числовых столбцов. Для категориальных — используйте .describe(include=\'object\').\nПроверка дубликатов и уникальности: .duplicated(), .drop_duplicates(), .nunique(), .unique().',
      },
      {
        id: 'topic-5-header-5',
        title: 'Выбор данных (Indexing and Selection) — критически важный навык',
        order: 5,
        content: 'Существует два основных интерфейса:\ndf[ ] (квадратные скобки):\nОдна скобка: df[\'col_name\'] → вернёт Series. df[[\'col1\', \'col2\']] → вернёт DataFrame.\nСрез: df[5:10] → срез по строкам (только по целочисленным позициям).\n.loc[] — выбор по МЕТКАМ (labels):\ndf.loc[10] → строка с индексом 10 (как df.iloc[10] если индекс числовой).\ndf.loc[5:10, \'col_name\'] → строки с метками от 5 до 10 включительно и столбец \'col_name\'.\ndf.loc[df[\'age\'] > 25, [\'name\', \'city\']] → булева индексация по строкам + выбор столбцов.\n.iloc[] — выбор по ЦЕЛОЧИСЛЕННЫМ ПОЗИЦИЯМ (integer-location):\ndf.iloc[5] → строка на позиции 5 (0-based).\ndf.iloc[5:10, 2:4] → строки с 5 по 9 и столбцы с 2 по 3 (как в срезах Python).\nВажно: .iloc[5:10] не включает 10-ю позицию.\nГлавное правило: .loc — для меток, .iloc — для позиций. Смешивать нельзя.',
      },
      {
        id: 'topic-5-header-6',
        title: 'Базовые операции с DataFrame',
        order: 6,
        content: 'Добавление/удаление столбцов:\ndf[\'new_col\'] = values — добавление.\ndf.drop(columns=[\'col1\', \'col2\'], inplace=True) — удаление. inplace=True меняет исходный df.\nПереименование: df.rename(columns={\'old\':\'new\'}, inplace=True).\nСортировка: df.sort_values(by=\'col\', ascending=False).\nТранспонирование: df.T.',
      },
      {
        id: 'topic-5-header-7',
        title: 'Группировка и агрегация (основы)',
        order: 7,
        content: 'df.groupby(\'key_column\') — разбивает DataFrame на группы по уникальным значениям указанного столбца.\nК группировке можно применять агрегирующие функции:\n.sum(), .mean(), .count(), .min(), .max(), .std().\n.agg({\'col1\':\'sum\', \'col2\':[\'mean\', \'std\']}) — продвинутая агрегация для разных столбцов.\nРезультат группировки — специальный объект, который можно преобразовать обратно в DataFrame.\n\nВывод по теме 5: Pandas — это основной инструмент для загрузки, хранения и первичного манипулирования табличными данными. Освоение DataFrame, методов инспекции .info()/.describe() и корректного выбора данных (.loc/.iloc) — фундамент для всего последующего анализа.',
      },
    ],
  },
  {
    id: 'topic-6',
    title: 'Тема 6: Операции очистки и предобработки данных',
    headers: [
      {
        id: 'topic-6-header-1',
        title: 'Цель этапа',
        order: 1,
        content: 'Преобразовать "сырые" данные в пригодный для анализа и моделирования формат. Решает проблемы: неконсистентность, беспорядок, некорректные типы данных.',
      },
      {
        id: 'topic-6-header-2',
        title: 'Работа со столбцами (Features/Columns)',
        order: 2,
        content: 'Приведение названий столбцов к единому стандарту:\nУдаление пробелов: df.columns = df.columns.str.strip().\nПриведение к нижнему регистру: df.columns = df.columns.str.lower().\nЗамена проблемных символов: df.columns = df.columns.str.replace(\' \', \'_\').\nВыбор и перестановка столбцов: df = df[[\'col1\', \'col3\', \'col2\']] — создает новый DataFrame в нужном порядке.',
      },
      {
        id: 'topic-6-header-3',
        title: 'Работа со строками (Observations/Rows)',
        order: 3,
        content: 'Фильтрация данных по условию: Использование булевых масок.\ndf = df[df[\'age\'] >= 18] — оставить только совершеннолетних.\ndf = df[(df[\'dept\'] == \'Sales\') & (df[\'salary\'] > 50000)] — комбинирование условий.\nУдаление дубликатов:\ndf.duplicated(subset=[\'col1\', \'col2\']) — поиск дубликатов по комбинации столбцов.\ndf.drop_duplicates(subset=[\'col1\', \'col2\'], keep=\'first\', inplace=True) — удаление. keep=\'last\' оставит последнюю копию.',
      },
      {
        id: 'topic-6-header-4',
        title: 'Преобразование типов данных (Type Casting)',
        order: 4,
        content: 'Проблема: Числа могут быть прочитаны как строки (объекты), даты — как строки.\nМетоды:\ndf[\'col\'] = df[\'col\'].astype(\'int64\') — явное преобразование.\npd.to_numeric(df[\'col\'], errors=\'coerce\') — умное преобразование в число. errors=\'coerce\' превратит ошибки в NaN.\npd.to_datetime(df[\'date_col\'], format=\'%d.%m.%Y\', errors=\'coerce\') — преобразование в дату. Всегда указывайте format, если известен.\ndf[\'col\'] = df[\'col\'].astype(\'category\') — для ограниченного набора строковых значений (экономия памяти, повышение скорости).',
      },
      {
        id: 'topic-6-header-5',
        title: 'Обработка строковых данных (String Operations)',
        order: 5,
        content: 'Доступ к строковым методам через .str:\ndf[\'name\'].str.upper() / .str.lower() / .str.title().\ndf[\'email\'].str.contains(\'gmail.com\') — поиск подстроки.\ndf[\'phone\'].str.replace(\'-\', \'\') — замена символов.\ndf[\'full_name\'].str.split(\' \', expand=True) — разделение строки на несколько столбцов.\nОбрезка пробелов: df[\'col\'] = df[\'col\'].str.strip().',
      },
      {
        id: 'topic-6-header-6',
        title: 'Создание новых признаков (Feature Engineering - основы)',
        order: 6,
        content: 'Извлечение частей из строк: Год из даты (df[\'year\'] = df[\'date\'].dt.year), домен из email.\nБиннинг (Binning): Превращение непрерывного признака (возраст) в категориальный (возрастные группы).\npd.cut(df[\'age\'], bins=[0, 18, 65, 100], labels=[\'child\', \'adult\', \'senior\']).\nПрименение функций: df[\'new\'] = df[\'col\'].apply(lambda x: x*2) или df[\'new\'] = df.apply(lambda row: row[\'a\']+row[\'b\'], axis=1).\n\nВывод по теме 6: Очистка — это последовательность рутинных, но жизненно важных операций: привидение столбцов в порядок, фильтрация строк, корректное задание типов и работа со строками. Качественно очищенные данные — залог успеха любого анализа.',
      },
    ],
  },
  {
    id: 'topic-7',
    title: 'Тема 7: Работа с пропущенными значениями и выбросами',
    headers: [
      {
        id: 'topic-7-header-1',
        title: 'Пропущенные значения (Missing Values)',
        order: 1,
        content: 'Обозначения: NaN (Not a Number, float), None (объект Python), NaT (Not a Time, для datetime).\nОбнаружение:\ndf.isna() или df.isnull() — булева матрица.\ndf.isna().sum() — количество пропусков в каждом столбце.\ndf.isna().sum().sum() — общее количество пропусков.',
      },
      {
        id: 'topic-7-header-2',
        title: 'Причины появления пропусков и стратегии обработки',
        order: 2,
        content: 'MCAR (Missing Completely at Random): Пропуск ни с чем не связан. Редко. Можно удалять без сильного смещения.\nMAR (Missing at Random): Пропуск связан с другими наблюдаемыми переменными. Нужна аккуратная обработка.\nMNAR (Missing Not at Random): Пропуск связан с самим пропущенным значением (например, богатые люди отказываются указывать доход). Самая проблемная ситуация.',
      },
      {
        id: 'topic-7-header-3',
        title: 'Методы обработки пропусков',
        order: 3,
        content: 'Удаление:\ndf.dropna(axis=0) — удаление строк, содержащих хотя бы один NaN.\ndf.dropna(axis=1, thresh=0.7*len(df)) — удаление столбцов, где более 30% значений пропущены.\nМинус: Потеря информации, может привести к смещенной выборке.\nЗаполнение (Imputation):\nКонстантой: df.fillna(0) или df.fillna(\'Unknown\').\nСтатистикой: df.fillna(df[\'col\'].mean()) или .median(), .mode()[0].\nДля категориальных данных — мода.\nДля числовых — медиана (устойчива к выбросам) или среднее.\nПродвинутые методы: Заполнение на основе других признаков, использование моделей (KNN, регрессия), интерполяция для временных рядов (df.interpolate()).\nВажно: Заполненные значения искажают распределение и дисперсию признака. Это нужно учитывать при моделировании.',
      },
      {
        id: 'topic-7-header-4',
        title: 'Выбросы (Outliers) — аномальные значения',
        order: 4,
        content: 'Причины: Ошибка измерения, редкое событие, естественная изменчивость.\nОбнаружение:\nВизуальное: Box-plot (ящик с усами), гистограмма, scatter plot.\nСтатистическое:\nПравило 3-х сигм (для нормального распределения): Выбросы за пределами mean ± 3*std.\nМетод межквартильного размаха (IQR, универсальный):\nQ1 = df[\'col\'].quantile(0.25)\nQ3 = df[\'col\'].quantile(0.75)\nIQR = Q3 - Q1\nНижняя граница: Q1 - 1.5 * IQR\nВерхняя граница: Q3 + 1.5 * IQR\nЗначения за этими границами считаются выбросами.',
      },
      {
        id: 'topic-7-header-5',
        title: 'Методы обработки выбросов',
        order: 5,
        content: 'Удаление: df = df[(df[\'col\'] >= lower_bound) & (df[\'col\'] <= upper_bound)].\nКаппирование (Winsorizing): Замена выбросов на граничные значения (например, все значения выше 99-го перцентиля приравниваются к значению 99-го перцентиля).\npython\ncap = df[\'col\'].quantile(0.99)\ndf[\'col_capped\'] = df[\'col\'].clip(upper=cap)\nПреобразование: Логарифмирование (np.log1p) может "прижать" очень большие значения.\nСоздание отдельной категории: Для моделей, основанных на деревьях, выброс можно выделить в отдельную метку (например, ">100k").\n\nВывод по теме 7: Пропуски и выбросы искажают статистики и работу моделей. Нет единственно правильного способа обработки. Выбор стратегии зависит от природы данных, доли аномалий и цели анализа. Всегда документируйте, что и как вы делали.',
      },
    ],
  },
  {
    id: 'topic-8',
    title: 'Тема 8: Визуализация данных: основные библиотеки и подходы',
    headers: [
      {
        id: 'topic-8-header-1',
        title: 'Цель визуализации',
        order: 1,
        content: 'Для аналитика (EDA): Увидеть паттерны, аномалии, распределения, связи. Визуализация — это инструмент мышления.\nДля презентации: Донести идею, вывод или историю до аудитории. Визуализация — это инструмент убеждения.',
      },
      {
        id: 'topic-8-header-2',
        title: 'Библиотеки и их экосистема',
        order: 2,
        content: 'Matplotlib: Фундаментальная, низкоуровневая библиотека. Даёт полный контроль над каждым элементом графика (как "рисование карандашом"). Часто используется как "движок" для других библиотек.\npyplot интерфейс (plt): Упрощённый, stateful-интерфейс.\nОбъектно-ориентированный интерфейс (OO): fig, ax = plt.subplots() — более гибкий и рекомендуемый для сложных графиков.\nSeaborn: Высокоуровневая библиотека на основе Matplotlib. Создана для статистической графики. Красивые стили по умолчанию, удобный API для сложных графиков (например, pairplot, heatmap).\nPandas Plotting: Простейшие графики прямо из Series/DataFrame (df.plot(), df[\'col\'].hist()). Основан на Matplotlib, хорош для быстрого просмотра.',
      },
      {
        id: 'topic-8-header-3',
        title: 'Основные типы графиков и их назначение',
        order: 3,
        content: 'Для одной числовой переменной (распределение):\nГистограмма (hist): Показывает частоту значений в интервалах (бинах). Позволяет оценить форму, центр, разброс, модальность, асимметрию.\nЯщик с усами (Box plot, boxplot): Показывает медиану, квартили (IQR) и выбросы. Отлично подходит для сравнения распределений между группами.\nПлотность распределения (KDE, kdeplot): Сглаженная оценка функции плотности.\nДля двух переменных (связи):\nТочечный график (Scatter plot, scatter): Связь между двумя непрерывными переменными. Показывает тренд, силу связи, кластеры.\nЛинейный график (Line plot, plot): Для временных рядов (изменение во времени).\nДля категориальных данных:\nСтолбчатая диаграмма (Bar chart, bar): Сравнение величин по категориям (например, средний доход по городам).\nГистограмма частот (Count plot, countplot в Seaborn): Показывает количество наблюдений в каждой категории (распределение категориального признака).\nДля многомерных данных:\nМатрица диаграмм рассеяния (Scatterplot Matrix, pairplot в Seaborn): Позволяет увидеть попарные связи между многими числовыми признаками.\nТепловая карта (Heatmap, heatmap): Отлично подходит для визуализации матрицы корреляций.',
      },
      {
        id: 'topic-8-header-4',
        title: 'Принципы эффективной визуализации',
        order: 4,
        content: 'Избегайте chartjunk (визуального мусора): Лишние сетки, 3D-эффекты для 2D-данных, неинформативные подписи.\nВыбирайте правильный тип графика под задачу (сравнение, распределение, связь, часть от целого).\nРабота с осями:\nВсегда подписывайте оси и давайте заголовок.\nНачинайте ось Y с 0 для столбчатых диаграмм (bar charts), иначе визуально искажаете разницу.\nИспользуйте логарифмическую шкалу (ax.set_yscale(\'log\')), если данные различаются на порядки.\nИспользование цвета:\nЦвет — мощный инструмент для выделения категорий.\nДля непрерывных данных используйте последовательные палитры (sequential colormaps).\nДля категориальных — качественные палитры (qualitative colormaps).\nУчитывайте дальтонизм (избегайте красно-зелёных комбинаций).',
      },
      {
        id: 'topic-8-header-5',
        title: 'Типичный рабочий процесс в Jupyter',
        order: 5,
        content: 'python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 1. Установка стиля (опционально)\nsns.set_style("whitegrid")\n# 2. Создание фигуры и осей\nfig, ax = plt.subplots(figsize=(10, 6)) # figsize в дюймах\n# 3. Построение графика (например, через Seaborn)\nsns.histplot(data=df, x=\'age\', kde=True, ax=ax)\n# 4. Кастомизация\nax.set_title(\'Распределение возраста\', fontsize=16)\nax.set_xlabel(\'Возраст (лет)\')\nax.set_ylabel(\'Количество\')\n# 5. Отображение\nplt.tight_layout() # Автоматическая подгонка отступов\nplt.show()\n\nВывод по теме 8: Визуализация — не просто "красивые картинки", а основной язык исследования данных. Умение выбрать правильный тип графика, построить его с помощью Matplotlib/Seaborn и корректно его подписать — ключевой навык аналитика как на этапе исследования (EDA), так и на этапе презентации результатов.',
      },
    ],
  },
  {
    id: 'topic-9',
    title: 'Тема 9: Исследовательский анализ данных (EDA)',
    headers: [
      {
        id: 'topic-9-header-1',
        title: 'Философия и цели EDA',
        order: 1,
        content: 'EDA (Exploratory Data Analysis) — это неформальный, интерактивный и творческий процесс изучения данных до построения формальных моделей или проверки гипотез.\nГлавная цель: Понять, "о чём говорят данные", выявить скрытые паттерны, аномалии, взаимосвязи и сформировать гипотезы. EDA — это "разговор" с данными.\nКлючевые принципы:\nВизуализация — первична: Графики часто показывают то, что не видят статистики.\nГибкость: Нет одного правильного пути. Методы меняются в зависимости от вопросов и природы данных.\nИтеративность: Каждый вопрос или находка порождает новые вопросы.',
      },
      {
        id: 'topic-9-header-2',
        title: 'Структура и этапы EDA (системный подход)',
        order: 2,
        content: 'Формирование понимания данных (Data Understanding):\nЗагрузка, первичный осмотр (shape, columns).\nИзучение метаинформации: что означает каждый признак (Data Dictionary).\nПроверка типов данных (dtypes) и их коррекция.\nОценка качества данных (Data Quality Assessment):\nПоиск пропусков (isna().sum()), дубликатов, неконсистентных значений (опечатки в категориальных признаках).\nВыявление явных выбросов на этом этапе.\nАнализ распределений (Univariate Analysis — анализ одной переменной):\nДля числовых признаков: Гистограмма + boxplot + описательные статистики (describe()). Оценка центра (mean, median), разброса (std, IQR), формы (skewness, modality).\nДля категориальных признаков: Bar chart / countplot + таблица частот (value_counts()). Оценка дисбаланса классов.\nАнализ взаимосвязей (Bivariate/Multivariate Analysis):\nЧисловой vs Числовой: Scatter plot (для 2 признаков) + матрица корреляций + heatmap (для многих признаков).\nКатегориальный vs Числовой: Boxplot (или violinplot) для сравнения распределений числовой величины по категориям. Использование агрегаций (groupby().mean()).\nКатегориальный vs Категориальный: Сводная таблица (pivot table) с подсчётом или долями, heatmap, stacked bar chart.\nФормирование и документирование инсайтов:\nОтветы на вопросы: "Что я узнал?", "Какие гипотезы можно сформулировать?", "Какие данные нужно очистить/преобразовать для моделирования?".',
      },
      {
        id: 'topic-9-header-3',
        title: 'Инструменты и техники для углублённого EDA',
        order: 3,
        content: 'pandas_profiling / ydata_profiling: Автоматическое создание развёрнутого отчёта (распределения, корреляции, пропуски, взаимодействия). Отличная точка входа.\nСводные таблицы (Pivot Tables): pd.pivot_table(df, index=\'cat_col\', values=\'num_col\', aggfunc=[\'mean\', \'count\']). Мощный инструмент для многомерного анализа.\nПарные графики (PairPlot): sns.pairplot(df, hue=\'target\') — позволяет сразу увидеть взаимосвязи и различия по целевой переменной.\nМатрица корреляций с аннотацией:\npython\ncorr_matrix = df.corr(numeric_only=True)\nsns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap=\'coolwarm\', center=0)\nИсследование целевой переменной (Target Variable Analysis): Самое важное. Как распределена целевая переменная? Сбалансированы ли классы? Как другие признаки с ней связаны?',
      },
      {
        id: 'topic-9-header-4',
        title: 'Типичные вопросы в ходе EDA',
        order: 4,
        content: 'Есть ли сезонность или тренд во временных данных?\nЕсть ли явные кластеры в данных на scatter plot?\nЕсть ли признаки мультиколлинеарности (сильной корреляции между предикторами)?\nНасколько признаки связаны с целевой переменной?\nКакие признаки имеют потенциал для feature engineering?\n\nВывод по теме 9: EDA — это основа основ аналитики. Это процесс, где вы становитесь "знакомы" со своим датасетом. Без качественного EDA последующее моделирование строится на зыбком фундаменте. Цель — не просто построить графики, а задавать правильные вопросы и находить истории в данных.',
      },
    ],
  },
  {
    id: 'topic-10',
    title: 'Тема 10: Работа с временными рядами в Pandas',
    headers: [
      {
        id: 'topic-10-header-1',
        title: 'Что такое временной ряд?',
        order: 1,
        content: 'Временной ряд (Time Series) — это последовательность данных, упорядоченных во времени, где время является ключевой независимой переменной.\nОсобенности: Наличие тренда, сезонности, цикличности и шума. Наблюдения часто автокоррелированы (значение в момент t зависит от значений в t-1, t-2...).',
      },
      {
        id: 'topic-10-header-2',
        title: 'Поддержка временных рядов в Pandas',
        order: 2,
        content: 'Специальный dtype: datetime64[ns].\nИндекс DatetimeIndex: Когда индекс DataFrame является datetime, открывается доступ к специальным методам ресемплинга и сдвига.\nСоздание: pd.to_datetime(), pd.date_range().\nВажно: При чтении данных указывать столбец с датой как индекс: df = pd.read_csv(\'data.csv\', parse_dates=[\'date_col\'], index_col=\'date_col\').',
      },
      {
        id: 'topic-10-header-3',
        title: 'Базовые операции с временными индексами',
        order: 3,
        content: 'Частичный доступ (Partial String Indexing): Можно срезать данные по году, месяцу и т.д.\npython\ndf[\'2023\']        # Все данные за 2023 год\ndf[\'2023-05\']     # Все данные за май 2023\ndf[\'2023-05-10\':\'2023-05-15\']  # Срез по датам\nИзвлечение атрибутов времени:\npython\ndf.index.year\ndf.index.month\ndf.index.dayofweek  # День недели (понедельник=0)\ndf.index.quarter\nСдвиг данных (Shifting / Lagging): df[\'value_lag1\'] = df[\'value\'].shift(periods=1) — получение значений предыдущего периода (лагов). Критично для прогнозных моделей.\nРазность (Differencing): df[\'value_diff\'] = df[\'value\'].diff(periods=1) — вычисление разности с предыдущим значением. Используется для устранения тренда.',
      },
      {
        id: 'topic-10-header-4',
        title: 'Ресемплинг (Resampling) и агрегация по времени',
        order: 4,
        content: 'Ресемплинг — это процесс изменения частоты временного ряда (например, с ежедневных данных на еженедельные).\nПонижающая дискретизация (Downsampling): Переход к более низкой частоте (дни → месяцы). Требует агрегации.\npython\n# Среднее значение за месяц\ndf.resample(\'M\').mean()\n# Сумма за неделю (понедельник - воскресенье)\ndf.resample(\'W\').sum()\nПовышающая дискретизация (Upsampling): Переход к более высокой частоте (часы → минуты). Требует интерполяции или заполнения пропусков.\npython\ndf.resample(\'D\').asfreq()          # Появится NaN для новых дат\ndf.resample(\'D\').ffill() / .bfill() # Заполнение вперед/назад\ndf.resample(\'D\').interpolate()      # Линейная интерполяция\nСкользящие (подвижные) окна (Rolling Windows): Вычисление статистик для "скользящего" окна фиксированного размера (полезно для сглаживания).\npython\ndf[\'rolling_mean_7\'] = df[\'value\'].rolling(window=7).mean()  # 7-дневное скользящее среднее\ndf[\'rolling_std_30\'] = df[\'value\'].rolling(window=30).std()  # 30-дневное скользящее СКО\nРасширяющиеся окна (Expanding Windows): Окно увеличивается от начала ряда до текущей точки (кумулятивные статистики).\npython\ndf[\'expanding_mean\'] = df[\'value\'].expanding().mean()',
      },
      {
        id: 'topic-10-header-5',
        title: 'Визуализация временных рядов',
        order: 5,
        content: 'Базовый line plot: df[\'value\'].plot(figsize=(12,6)).\nНесколько рядов на одном графике для сравнения.\nВизуализация компонент: Разложение ряда на тренд, сезонность и остаток (можно с помощью statsmodels).\nГрафики скользящей статистики для выявления тренда на зашумленных данных.',
      },
      {
        id: 'topic-10-header-6',
        title: 'Поиск тренда и сезонности',
        order: 6,
        content: 'Тренд: Долгосрочное направленное движение данных (рост, спад, стагнация). Выявляется скользящим средним или линейной регрессией по времени.\nСезонность: Периодические колебания с фиксированной частотой (сутки, неделя, год). Выявляется разложением ряда или анализом автокорреляционной функции.\n\nВывод по теме 10: Работа с временными рядами требует особого подхода из-за присущей им структуры и зависимостей. Pandas предоставляет мощный и удобный инструментарий для манипуляций с временными данными: от базового парсинга дат до сложного ресемплинга и вычисления скользящих статистик, которые являются основой для дальнейшего анализа и прогнозирования.',
      },
    ],
  },
  {
    id: 'topic-11',
    title: 'Тема 11: Основы статистики для анализа данных',
    headers: [
      {
        id: 'topic-11-header-1',
        title: 'Зачем аналитику статистика?',
        order: 1,
        content: 'Чтобы делать обоснованные выводы из данных, а не строить догадки.\nЧтобы корректно интерпретировать результаты моделей машинного обучения.\nЧтобы понимать, являются ли обнаруженные паттерны статистически значимыми или они возникли случайно.',
      },
      {
        id: 'topic-11-header-2',
        title: 'Описательная статистика (Descriptive Statistics)',
        order: 2,
        content: 'Меры центральной тенденции (Central Tendency):\nСреднее (Mean): Сумма всех значений, делённая на их количество. Чувствительно к выбросам.\nМедиана (Median): Значение, которое делит упорядоченный набор пополам. Устойчивая к выбросам мера.\nМода (Mode): Наиболее часто встречающееся значение.\nМеры изменчивости (Variability / Spread):\nРазмах (Range): Разность между максимальным и минимальным значениями. Очень чувствителен к выбросам.\nДисперсия (Variance): Среднее арифметическое квадратов отклонений от среднего. Var(X) = Σ(x_i - mean)² / (n-1) (несмещённая оценка).\nСтандартное отклонение (Standard Deviation, SD, σ): Квадратный корень из дисперсии. Измеряется в тех же единицах, что и данные. Показывает "типичное" отклонение от среднего.\nМежквартильный размах (Interquartile Range, IQR): IQR = Q3 - Q1. Устойчив к выбросам. Основа для boxplot.\nМеры формы распределения (Shape):\nАсимметрия (Skewness): Показывает, насколько распределение несимметрично.\nskew > 0 — правый хвост длиннее (положительная асимметрия, мода < медиана < среднее).\nskew < 0 — левый хвост длиннее (отрицательная асимметрия).\nЭксцесс (Kurtosis): Показывает "остроту" пика и тяжесть хвостов по сравнению с нормальным распределением.\nkurtosis > 0 — тяжёлые хвосты (распределение имеет больше выбросов, чем нормальное).\nkurtosis < 0 — легкие хвосты.',
      },
      {
        id: 'topic-11-header-3',
        title: 'Основы теории вероятностей для анализа',
        order: 3,
        content: 'Распределения вероятностей:\nНормальное (Гауссово) распределение: Колоколообразная кривая. Многие статистические методы предполагают нормальность данных. Определяется средним (μ) и стандартным отклонением (σ).\nРавномерное распределение: Все значения равновероятны.\nБиномиальное распределение: Число "успехов" в серии независимых испытаний.\nЦентральная предельная теорема (ЦПТ): При достаточно большом объёме выборки (n > 30) выборочное распределение среднего будет стремиться к нормальному распределению независимо от формы исходного распределения данных. Фундамент для статистического вывода.',
      },
      {
        id: 'topic-11-header-4',
        title: 'Статистический вывод (Statistical Inference)',
        order: 4,
        content: 'Генеральная совокупность (Population) и Выборка (Sample): Мы почти всегда работаем с выборкой, чтобы делать выводы о популяции.\nВыборочная статистика (Sample Statistic, например, x̄ — выборочное среднее) — это оценка параметра популяции (Population Parameter, μ — истинное среднее).\nРаспределение выборочного среднего: В соответствии с ЦПТ, оно нормально со средним μ и стандартным отклонением σ / √n (стандартная ошибка среднего, SEM).',
      },
      {
        id: 'topic-11-header-5',
        title: 'Интервальное оценивание (Доверительные интервалы)',
        order: 5,
        content: 'Доверительный интервал (Confidence Interval, CI) — это диапазон значений, который с заданной вероятностью (уровнем доверия, например, 95%) содержит истинный параметр популяции.\nФормула для среднего (при известном σ или большом n): x̄ ± Z*(σ/√n), где Z — Z-оценка, соответствующая уровню доверия (для 95% это ~1.96).\nИнтерпретация: Если мы построим 100 таких 95% доверительных интервалов по разным выборкам из одной популяции, то примерно 95 из них будут содержать истинное среднее μ.',
      },
      {
        id: 'topic-11-header-6',
        title: 'Ключевые концепции проверки гипотез (введение)',
        order: 6,
        content: 'Нулевая гипотеза (H₀): Консервативное предположение, которое мы пытаемся опровергнуть (например, "лекарство неэффективно", "средние двух групп равны").\nАльтернативная гипотеза (H₁ или Hₐ): Утверждение, которое мы хотим доказать ("лекарство эффективно", "средние различаются").\np-value (p-уровень значимости): Вероятность получить наблюдаемые или ещё более крайние результаты при условии, что нулевая гипотеза верна.\nМалое p-value (< α, где α — уровень значимости, обычно 0.05) говорит о том, что наблюдаемые данные маловероятны, если H₀ верна. Это дает основание отвергнуть H₀.\nБольшое p-value: Нет достаточных оснований отвергнуть H₀.\nОшибки:\nОшибка I рода (False Positive): Отвергнуть верную H₀. Вероятность = α.\nОшибка II рода (False Negative): Не отвергнуть ложную H₀. Вероятность = β.\n\nВывод по теме 11: Статистика — это язык, на котором говорит анализ данных. Понимание описательных статистик позволяет резюмировать данные, а знание основ статистического вывода (ЦПТ, доверительные интервалы, p-value) позволяет корректно переносить выводы с выборки на всю генеральную совокупность и принимать обоснованные решения.',
      },
    ],
  },
  {
    id: 'topic-12',
    title: 'Тема 12: Корреляция и ковариация. Проверка гипотез',
    headers: [
      {
        id: 'topic-12-header-1',
        title: 'Ковариация (Covariance)',
        order: 1,
        content: 'Ковариация измеряет направление линейной связи между двумя переменными.\nФормула: Cov(X,Y) = Σ[(x_i - x̄)(y_i - ȳ)] / (n-1)\nИнтерпретация знака:\nCov > 0 — Прямая связь (с ростом X растёт Y).\nCov < 0 — Обратная связь (с ростом X уменьшается Y).\nCov ≈ 0 — Линейная связь отсутствует.\nНедостаток: Величина ковариации зависит от единиц измерения переменных. Нельзя сказать, сильная связь или слабая.',
      },
      {
        id: 'topic-12-header-2',
        title: 'Корреляция Пирсона (Pearson Correlation Coefficient)',
        order: 2,
        content: 'Коэффициент корреляции Пирсона (r) — это стандартизированная ковариация. Не имеет размерности.\nФормула: r = Cov(X,Y) / (σ_x * σ_y)\nСвойства:\n-1 ≤ r ≤ 1\nr = 1 — Идеальная прямая линейная связь.\nr = -1 — Идеальная обратная линейная связь.\nr = 0 — Отсутствие линейной связи (но может быть нелинейная!).\nИнтерпретация величины (примерная):\n|r| < 0.3 — слабая связь.\n0.3 ≤ |r| < 0.7 — умеренная связь.\n|r| ≥ 0.7 — сильная связь.\nВажное ограничение: Корреляция измеряет только линейную связь. Она не улавливает сложные нелинейные зависимости.',
      },
      {
        id: 'topic-12-header-3',
        title: 'Проверка значимости корреляции',
        order: 3,
        content: 'Наблюдаемый коэффициент корреляции r вычислен по выборке. Нужно проверить, значимо ли он отличается от нуля в популяции.\nНулевая гипотеза H₀: ρ = 0 (истинная корреляция в популяции равна нулю).\nАльтернативная гипотеза H₁: ρ ≠ 0 (корреляция в популяции существует).\nИспользуется t-тест для корреляции: t = r * √(n-2) / √(1 - r²).\nПо значению t и объёму выборки вычисляется p-value.\nВывод: Если p-value < α (например, 0.05), то отвергаем H₀ и делаем вывод, что корреляция статистически значима.',
      },
      {
        id: 'topic-12-header-4',
        title: 'Общая логика проверки статистических гипотез (расширение)',
        order: 4,
        content: 'Формулировка гипотез: H₀ и H₁.\nВыбор уровня значимости (α): Вероятность совершить ошибку I рода (обычно 0.05 или 0.01).\nВыбор статистического критерия (теста) в зависимости от типа данных и задачи:\nt-критерий Стьюдента: Сравнение средних (1 выборка с константой, 2 независимые выборки, 2 зависимые выборки).\nКритерий Хи-квадрат (χ²): Проверка связи между категориальными переменными или соответствия распределения.\nКритерий Манна-Уитни (U-тест): Непараметрический аналог t-теста для 2 независимых выборок (не требует нормальности).\nANOVA: Сравнение средних в трёх и более группах.\nРасчёт наблюдаемого значения статистики теста по выборочным данным.\nОпределение p-value: Вероятность получить такое или более крайнее значение статистики при условии истинности H₀.\nПринятие решения:\np-value ≤ α → Отвергаем H₀ в пользу H₁. Результат статистически значим.\np-value > α → Не отвергаем H₀. Статистически значимых доказательств в пользу H₁ нет.',
      },
      {
        id: 'topic-12-header-5',
        title: 'Практические аспекты и предостережения',
        order: 5,
        content: 'Корреляция ≠ Причинно-следственная связь: Сильная корреляция не означает, что одна переменная вызывает изменения в другой. Возможны:\nСлучайность.\nВлияние третьей, скрытой переменной (confounding variable).\nОбратная причинность.\nПроблема множественного тестирования: Если провести много статистических тестов, возрастает вероятность случайно получить значимый результат (ошибка I рода). Нужны поправки (например, Бонферрони).\nСтатистическая значимость vs Практическая значимость: Очень слабый эффект (например, r=0.1) может быть статистически значимым на огромной выборке, но абсолютно бесполезным на практике. Всегда смотрите на величину эффекта (например, сам коэффициент r).\n\nВывод по теме 12: Корреляция — это первый шаг к анализу взаимосвязей, но её интерпретация требует осторожности. Проверка гипотез — это формальный механизм, позволяющий отделить систематические эффекты от случайного шума в данных, но его результаты нужно интерпретировать в контексте предметной области и размера эффекта.',
      },
    ],
  },
  {
    id: 'topic-13',
    title: 'Тема 13: Основы машинного обучения: постановка задачи и подготовка данных',
    headers: [
      {
        id: 'topic-13-header-1',
        title: 'Что такое машинное обучение (ML)?',
        order: 1,
        content: 'Классическое определение (Артур Самуэль): "Область исследований, которая дает компьютерам способность учиться без явного программирования".\nСовременное определение: Создание моделей (алгоритмов), которые на основе обучающих данных могут выявлять закономерности (паттерны) и использовать их для прогнозирования на новых данных или принятия решений.',
      },
      {
        id: 'topic-13-header-2',
        title: 'Типы задач машинного обучения',
        order: 2,
        content: 'Обучение с учителем (Supervised Learning): В данных есть целевая переменная (target, label), которую модель учится предсказывать.\nКлассификация (Classification): Целевая переменная — категория (класс). Примеры: спам/не спам, диагноз (A/B/C), вид ириса. Основные алгоритмы: логистическая регрессия, решающие деревья, метод k-ближайших соседей (KNN).\nРегрессия (Regression): Целевая переменная — непрерывное число. Примеры: цена дома, температура завтра, время доставки. Основные алгоритмы: линейная регрессия, деревья регрессии.\nОбучение без учителя (Unsupervised Learning): В данных нет целевой переменной. Задача — найти скрытую структуру в данных.\nКластеризация (Clustering): Разбиение данных на группы (кластеры) так, чтобы объекты внутри группы были похожи, а между группами — различны. Примеры: сегментация клиентов, группировка документов. Основной алгоритм: K-Means.\nСнижение размерности (Dimensionality Reduction): Упрощение данных за счёт уменьшения количества признаков при сохранении максимального количества информации. Используется для визуализации или ускорения работы моделей. Основной метод: PCA.\nОбучение с подкреплением (Reinforcement Learning): Агент учится, взаимодействуя со средой и получая награду за правильные действия. (В вашем курсе, скорее всего, не рассматривается детально).',
      },
      {
        id: 'topic-13-header-3',
        title: 'Ключевые понятия и этапы ML-проекта',
        order: 3,
        content: 'Признаки (Features): Входные переменные (столбцы в DataFrame, кроме целевого). Это "сырьё" для модели.\nЦелевая переменная (Target/Label): То, что мы предсказываем.\nМодель (Model): Алгоритм + найденные в процессе обучения параметры.\nОбучение (Training/Fitting): Процесс "подгонки" модели под обучающие данные (нахождение параметров, минимизирующих ошибку).\nПрогноз (Prediction/Inference): Применение обученной модели к новым данным для получения ответа.\nЭтапы проекта:\nПостановка задачи: Перевести бизнес-проблему ("хочу увеличить продажи") в задачу ML ("предсказать, какой клиент купит").\nСбор и подготовка данных (80% времени): EDA, очистка, feature engineering.\nРазделение данных: X_train, X_test, y_train, y_test (часто также X_val, y_val).\nВыбор и обучение модели: Подбор алгоритма, его обучение на train-выборке.\nОценка модели: Тестирование на test-выборке (которую модель не видела при обучении).\nНастройка гиперпараметров: Поиск лучших настроек алгоритма на валидационной выборке.\nИнтерпретация и внедрение.',
      },
      {
        id: 'topic-13-header-4',
        title: 'Подготовка данных для ML (Feature Engineering для моделей)',
        order: 4,
        content: 'Кодирование категориальных признаков:\nLabel Encoding: Присвоение каждой категории числа (0, 1, 2...). Плохо для линейных моделей, если порядок не имеет смысла (например, "Москва", "Париж", "Токио").\nOne-Hot Encoding (OHE): Создание нового бинарного (0/1) признака для каждой категории. Используется для номинальных признаков (без порядка). pd.get_dummies().\nМасштабирование числовых признаков: Критически важно для алгоритмов, использующих расстояние (KNN, SVM) или градиентный спуск (линейная/логистическая регрессия).\nStandardScaler: Приводит данные к распределению с mean=0 и std=1. z = (x - u) / s. Лучший выбор по умолчанию.\nMinMaxScaler: Приводит все значения к диапазону [0, 1]. X_scaled = (X - X.min) / (X.max - X.min). Чувствителен к выбросам.\nВажно: Масштабировать только на train-выборке, а затем применять те же параметры (среднее/стандартное отклонение) к test-выборке.',
      },
      {
        id: 'topic-13-header-5',
        title: 'Разделение данных и проблема "утечки данных" (Data Leakage)',
        order: 5,
        content: 'Принцип: Модель не должна никоим образом "видеть" данные из тестовой выборки в процессе обучения.\nТипичная ошибка: Масштабировать или заполнять пропуски по всему датасету до разделения. Правильно: разделить, затем все преобразования делать, обучаясь только на X_train.\nФункция train_test_split: from sklearn.model_selection import train_test_split\npython\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nrandom_state — для воспроизводимости результатов.\nstratify=y — для сохранения пропорций классов в разбиении (важно для несбалансированных данных).\n\nВывод по теме 13: Машинное обучение — это процесс превращения данных в предсказательные модели. Ключ к успеху — четкая постановка задачи, тщательная подготовка данных (кодирование, масштабирование) и строгое разделение данных на обучающую и тестовую выборки для честной оценки.',
      },
    ],
  },
  {
    id: 'topic-14',
    title: 'Тема 14: Линейная и логистическая регрессия',
    headers: [
      {
        id: 'topic-14-header-1',
        title: 'Линейная регрессия (Linear Regression)',
        order: 1,
        content: 'Идея: Смоделировать целевую переменную y как линейную комбинацию признаков X плюс свободный член (intercept).\nМатематическая модель (для одного признака): y = w₁*x + w₀\nw₀ (intercept) — смещение, точка пересечения с осью Y.\nw₁ (coefficient) — вес (коэффициент) признака. Показывает, на сколько изменится y при изменении x на 1.\nМногомерный случай: y = w₀ + w₁*x₁ + w₂*x₂ + ... + wₙ*xₙ\nОбучение (как найти w?): Метод наименьших квадратов (Ordinary Least Squares, OLS). Ищем такие веса w, чтобы минимизировать функцию потерь (Loss Function) — сумму квадратов ошибок (разниц между предсказанием и истинным значением) по всем объектам обучающей выборки.',
      },
      {
        id: 'topic-14-header-2',
        title: 'Оценка качества регрессии',
        order: 2,
        content: 'Метрики (необходимо знать несколько):\nMAE (Mean Absolute Error): Средняя абсолютная ошибка. MAE = (1/n) * Σ|y_i - ŷ_i|. Легко интерпретировать ("в среднем ошибаемся на ±X единиц").\nMSE (Mean Squared Error): Среднеквадратичная ошибка. MSE = (1/n) * Σ(y_i - ŷ_i)². Сильнее штрафует за большие ошибки.\nRMSE (Root Mean Squared Error): Корень из MSE. RMSE = sqrt(MSE). Измеряется в тех же единицах, что и y.\nR² (коэффициент детерминации): Доля дисперсии целевой переменной, объясненная моделью. R² = 1 - (SS_res / SS_tot), где SS_res — сумма квадратов остатков, SS_tot — общая сумма квадратов.\nR² = 1 — идеальная модель.\nR² = 0 — модель не лучше, чем предсказание константой (средним значением).\nR² может быть отрицательным на тестовых данных, если модель работает ужасно.',
      },
      {
        id: 'topic-14-header-3',
        title: 'Логистическая регрессия (Logistic Regression)',
        order: 3,
        content: 'Идея: Несмотря на название, это алгоритм для бинарной классификации (2 класса, например, 0 и 1).\nВыход модели: Не прямое предсказание класса, а вероятность принадлежности к классу 1 (p = P(y=1|X)).\nМатематика: Применяет логистическую (сигмоидную) функцию к линейной комбинации признаков, чтобы "сжать" результат в интервал [0, 1].\np = 1 / (1 + e^-(w₀ + w₁*x₁ + ...))\nСигмоида превращает любое число в вероятность.\nПринятие решения: Если p >= 0.5 (порог по умолчанию), предсказываем класс 1, иначе — 0. Порог можно менять в зависимости от задачи.',
      },
      {
        id: 'topic-14-header-4',
        title: 'Оценка качества классификации',
        order: 4,
        content: 'Матрица ошибок (Confusion Matrix): Основная таблица для анализа.\ntext\n           | Предсказано 0 | Предсказано 1\n------------|---------------|--------------\nИстинно 0   | TN            | FP\nИстинно 1   | FN            | TP\nTP (True Positive): Верно предсказали "1".\nTN (True Negative): Верно предсказали "0".\nFP (False Positive): Ошибочно предсказали "1" (ошибка I рода).\nFN (False Negative): Ошибочно предсказали "0" (ошибка II рода).\nМетрики, производные от матрицы:\nAccuracy (Доля верных ответов): (TP+TN) / (TP+TN+FP+FN). Не работает на несбалансированных данных.\nPrecision (Точность): TP / (TP+FP). Какая доля объектов, предсказанных как "1", действительно "1"? (Качество позитивного прогноза).\nRecall (Полнота, Sensitivity): TP / (TP+FN). Какая доле реальных "1" была предсказана правильно? (Способность находить все позитивные случаи).\nF1-score: Гармоническое среднее Precision и Recall. F1 = 2 * (Precision * Recall) / (Precision + Recall). Баланс между двумя метриками.\nROC-кривая и AUC-ROC:\nROC-кривая: Строится для разных порогов классификации. По оси X — FPR (False Positive Rate = FP/(FP+TN)), по оси Y — TPR (True Positive Rate = Recall).\nAUC-ROC (Area Under Curve): Площадь под ROC-кривой. Показывает, насколько модель способна отделять классы. Значение от 0.5 (случайный угадыватель) до 1.0 (идеальная модель).',
      },
      {
        id: 'topic-14-header-5',
        title: 'Регуляризация: борьба с переобучением',
        order: 5,
        content: 'Проблема: Модель может стать слишком сложной и "запомнить" обучающие данные (шум), вместо того чтобы изучить общие закономерности (переобучение, overfitting). Проявляется в виде огромных весов.\nРешение: Добавить в функцию потерь штраф за большие веса.\nL1-регуляризация (Lasso): Штрафует за сумму модулей весов. Может обнулять веса неважных признаков (отбор признаков).\nL2-регуляризация (Ridge): Штрафует за сумму квадратов весов. Распределяет вес между всеми признаками, уменьшая их величину.\nElasticNet: Комбинация L1 и L2.\n\nВывод по теме 14: Линейная и логистическая регрессия — это фундаментальные, интерпретируемые и мощные алгоритмы. Линейная регрессия предсказывает числа, логистическая — вероятности классов. Ключ к их успешному применению — правильная подготовка данных (масштабирование для линейной регрессии), понимание метрик оценки и применение регуляризации для борьбы с переобучением.',
      },
    ],
  },
  {
    id: 'topic-15',
    title: 'Тема 15: Методы кластеризации данных',
    headers: [
      {
        id: 'topic-15-header-1',
        title: 'Суть кластеризации',
        order: 1,
        content: 'Задача без учителя: Разделить данные на группы (кластеры) так, чтобы:\nОбъекты внутри одного кластера были максимально похожи (высокая внутрикластерная схожесть).\nОбъекты из разных кластеров были максимально различны (низкая межкластерная схожесть).\nВажно: В кластеризации нет "правильных" ответов. Это инструмент для исследования, и результат зависит от выбора алгоритма, метрики и параметров.',
      },
      {
        id: 'topic-15-header-2',
        title: 'Алгоритм K-Means (K-средних)',
        order: 2,
        content: 'Идея: Задать число кластеров K. Алгоритм итеративно находит K центроидов (точек — центров кластеров) и присваивает каждому объекту кластер ближайшего к нему центроида.\nШаги алгоритма:\nИнициализация: Случайный выбор K точек в качестве начальных центроидов.\nНазначение: Каждому объекту назначается кластер, центроид которого ему ближе всего (по евклидову расстоянию).\nОбновление: Для каждого кластера пересчитывается центроид как среднее арифметическое всех объектов этого кластера.\nПовторение шагов 2 и 3 до сходимости (когда центроиды перестают значительно меняться или меняются назначения).\nКритерий качества (инерция, inertia): Сумма квадратов расстояний от каждого объекта до центроида его кластера. Алгоритм стремится минимизировать инерцию.',
      },
      {
        id: 'topic-15-header-3',
        title: 'Выбор числа кластеров (K)',
        order: 3,
        content: 'Метод локтя (Elbow Method): Построение графика зависимости инерции от числа кластеров K. Ищем "локоть" — точку, после которой инерция падает не так резко. Это значение K часто берут за оптимальное.\nСубъективная интерпретация: Лучшее K — то, которое дает осмысленные с точки зрения предметной области кластеры.',
      },
      {
        id: 'topic-15-header-4',
        title: 'Особенности и проблемы K-Means',
        order: 4,
        content: 'Требует указания K (числа кластеров) заранее.\nЧувствительность к инициализации (случайному выбору начальных центроидов). Решение: n_init параметр (запуск алгоритма несколько раз с разной инициализацией и выбор лучшего результата).\nЧувствительность к выбросам, так как центроид — это среднее.\nПредполагает, что кластеры имеют сферическую форму и примерно одинаковый размер (из-за использования евклидова расстояния). Не справится с кластерами сложной формы.\nТребует масштабирования признаков, так как использует евклидово расстояние.',
      },
      {
        id: 'topic-15-header-5',
        title: 'Иерархическая кластеризация',
        order: 5,
        content: 'Идея: Построить дерево кластеров (дендрограмма), где в корне — один большой кластер (все данные), а в листьях — отдельные объекты.\nПодходы:\nАгломеративная (снизу вверх): Начинаем с отдельных объектов, последовательно объединяем самые близкие кластеры.\nДивизимная (сверху в down): Начинаем с одного кластера, последовательно разделяем.\nСпособы определения расстояния между кластерами (linkage):\nSingle linkage: Расстояние между ближайшими точками двух кластеров. Склонен создавать длинные "цепочки".\nComplete linkage: Расстояние между самыми дальними точками. Склонен создавать компактные кластеры.\nAverage linkage: Среднее расстояние между всеми парами точек из двух кластеров.\nРезультат: Дендрограмма. Выбор числа кластеров осуществляется "разрезанием" дендрограммы на нужной высоте.',
      },
      {
        id: 'topic-15-header-6',
        title: 'Оценка качества кластеризации',
        order: 6,
        content: 'Внутренние метрики (не требуют истинных меток):\nSilhouette Score (силуэтный коэффициент): Оценивает, насколько объект похож на свой кластер по сравнению с другими кластерами. Диапазон от -1 до 1. Чем выше, тем лучше.\nCalinski-Harabasz Index (Variance Ratio Criterion): Отношение межклассовой дисперсии к внутриклассовой. Чем выше, тем лучше разделены кластеры.\nВнешние метрики (если истинные метки известны — для тестирования):\nAdjusted Rand Index (ARI), Adjusted Mutual Information (AMI): Измеряют сходство между истинной разметкой и предсказанной кластеризацией, с поправкой на случайность.\n\nВывод по теме 15: Кластеризация — мощный инструмент для исследования структуры данных. K-Means — простой и популярный, но имеет ряд ограничений. Иерархическая кластеризация дает более наглядное представление (дендрограмма), но может быть медленной на больших данных. Выбор метода и параметров должен быть осмысленным и опираться на визуализацию и метрики.',
      },
    ],
  },
  {
    id: 'topic-16',
    title: 'Тема 16: Снижение размерности данных: PCA и другие методы',
    headers: [
      {
        id: 'topic-16-header-1',
        title: 'Зачем нужно снижение размерности?',
        order: 1,
        content: 'Проклятие размерности (Curse of Dimensionality): С ростом числа признаков данные становятся "разреженными", расстояния между точками перестают быть информативными, сложность моделей растет.\nЦели:\nВизуализация: Проецируем данные на 2D/3D, чтобы увидеть структуру.\nУскорение алгоритмов (уменьшение объема вычислений).\nУлучшение качества моделей: Удаление шума и избыточных признаков.\nСжатие данных.',
      },
      {
        id: 'topic-16-header-2',
        title: 'Метод главных компонент (Principal Component Analysis, PCA)',
        order: 2,
        content: 'Идея: Найти новые оси (главные компоненты), такие что:\nПервая главная компонента (PC1) объясняет максимальную дисперсию в данных.\nВторая главная компонента (PC2) ортогональна первой и объясняет максимальную дисперсию из оставшейся, и так далее.\nМатематика: Линейное преобразование. Новые оси — это собственные векторы ковариационной матрицы исходных данных, отсортированные по убыванию собственных значений (которые показывают долю объясненной дисперсии).\nСвойства главных компонент:\nОртогональны (некоррелированы).\nИх количество равно min(n_samples, n_features).\nОни являются линейными комбинациями исходных признаков.',
      },
      {
        id: 'topic-16-header-3',
        title: 'Этапы применения PCA',
        order: 3,
        content: 'Масштабирование данных (обязательно!): PCA чувствителен к масштабу. Если признаки в разных единицах, нужно применить StandardScaler.\nВычисление ковариационной матрицы (или SVD разложения).\nВычисление собственных векторов и значений.\nСортировка собственных векторов по убыванию собственных значений.\nВыбор числа компонент k:\nПо доле объясненной дисперсии (Explained Variance Ratio): Обычно выбирают k, чтобы сохранить, например, 95% дисперсии.\nПо графику "каменистой осыпи" (Scree Plot): График собственных значений. Ищем "излом", после которого значения падают медленно.\nПроецирование данных на выбранные k главных компонент: X_reduced = X_scaled @ W_k, где W_k — матрица из первых k собственных векторов.',
      },
      {
        id: 'topic-16-header-4',
        title: 'Интерпретируемость PCA',
        order: 4,
        content: 'Плюс: Новые признаки (PC) некоррелированы, что полезно для некоторых моделей.\nМинус: Главные компоненты — это абстрактные линейные комбинации исходных признаков. Их сложно содержательно интерпретировать (например, что такое PC1 = 0.5*рост - 0.1*вес + 0.8*возраст?).\nГрафик нагрузки (Loading Plot): Показывает вклад каждого исходного признака в главные компоненты. Помогает понять, из чего "состоят" PC.',
      },
      {
        id: 'topic-16-header-5',
        title: 't-SNE (t-distributed Stochastic Neighbor Embedding) — для визуализации',
        order: 5,
        content: 'Идея: Нелинейный метод, который фокусируется на сохранении локальных расстояний (похожие объекты в исходном пространстве должны быть близки в новом).\nОсобенности:\nОтлично подходит для визуализации кластеров в 2D/3D.\nРезультаты запусков могут отличаться (стохастический алгоритм).\nМедленный на больших данных.\nНе сохраняет глобальную структуру (расстояния между кластерами на графике t-SNE могут ничего не значить).\nНельзя использовать для feature extraction в моделях, только для визуализации.\nПрименение: После того, как PCA сократил размерность до ~50, можно применить t-SNE для визуализации в 2D.',
      },
      {
        id: 'topic-16-header-6',
        title: 'Другие методы',
        order: 6,
        content: 'LDA (Linear Discriminant Analysis): Метод с учителем. Ищет оси, которые максимизируют разделение между классами (в отличие от PCA, который максимизирует дисперсию). Используется как для снижения размерности, так и для классификации.\nUMAP (Uniform Manifold Approximation and Projection): Современный нелинейный метод, конкурент t-SNE. Часто лучше сохраняет глобальную структуру и работает быстрее.\n\nВывод по теме 16: Снижение размерности — критически важный этап для работы с высокомерными данными. PCA — основной линейный метод для сокращения числа признаков с минимальной потерей информации, но его компоненты сложно интерпретировать. t-SNE — прекрасный инструмент для визуализации сложных структур и кластеров в 2D. Выбор метода зависит от цели: ускорение моделей (PCA) или исследовательская визуализация (t-SNE/UMAP).',
      },
    ],
  },
];
