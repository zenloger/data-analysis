Конспект по курсу «Введение в анализ данных с Python»
Тема 1: Введение в анализ данных. Жизненный цикл данных 
1. Что такое данные и их типы (ключевое для всего курса)
Данные — это факты, наблюдения или измерения, представленные в формализованном виде.
Типы данных по структуре:
Структурированные: Таблицы (SQL, Excel). Имеют четкую схему: строки (наблюдения) и столбцы (признаки). Пример: данные о транзакциях.
Полуструктурированные: Имеют нечеткую или гибкую схему (JSON, XML, HTML). Пример: лог-файлы, ответы API.
Неструктурированные: Не имеют предопределенной табличной формы. Текст, изображения, аудио, видео. Анализ требует специфических методов (NLP, Computer Vision).
2. Уровни аналитики (по возрастанию сложности и ценности)
Descriptive Analytics (Что произошло?):
Суть: Описание исторических данных. Агрегация и визуализация.
Инструменты: SQL, сводные таблицы, дашборды (Tableau, Power BI).
Пример: «Вчера продажи в регионе А выросли на 15%».
Diagnostic Analytics (Почему это произошло?):
Суть: Поиск причинно-следственных связей, «копание» в данных.
Методы: Детализация (drill-down), корреляционный анализ, выявление аномалий.
Пример: «Рост продаж в регионе А связан с запуском рекламной кампании Х».
Predictive Analytics (Что произойдет?):
Суть: Прогнозирование будущих событий на основе исторических паттернов.
Методы: Регрессия, машинное обучение, временные ряды.
Пример: «На следующей неделе прогнозируется падение продаж товара Y на 7%».
Prescriptive Analytics (Как сделать, чтобы...?):
Суть: Рекомендация оптимальных действий для достижения цели.
Методы: Оптимизация, A/B-тестирование, симуляции.
Пример: «Чтобы увеличить прибыль на 10%, рекомендуем снизить цену на товар Z на 5% и увеличить бюджет на канал В».
3. Детальный разбор CRISP-DM (с акцентом на практику)
Каждый этап имеет свои ключевые выходные артефакты.
Business Understanding:
Ключевой вопрос: Какую бизнес-проблему мы решаем?
Артефакты: Устное определение цели (Goal), критерии успеха (Success Criteria), план проекта.
Data Understanding:
Ключевые действия: Сбор данных из всех доступных источников (базы данных, CSV, API). Первичный осмотр (Initial Data Exploration): df.head(), df.info(), df.describe(). Поиск явных проблем.
Артефакты: Описание данных (Data Dictionary), список проблем качества.
Data Preparation (самый длинный этап):
Основные задачи:
Объединение данных (Merging/Joining): Сборка финального датасета из разных таблиц.
Очистка (Cleaning): Работа с пропусками, дубликатами, неконсистентными записями (например, «М» и «Муж.»).
Преобразование (Transformation): Создание новых признаков (Feature Engineering), нормализация, категоризация (binning).
Артефакт: Чистый, готовый к анализу датасет.
Modeling:
Выбор алгоритма зависит от типа задачи (классификация, регрессия, кластеризация).
Разделение данных на обучающую (train) и тестовую (test) выборки до обучения для адекватной оценки.
Evaluation:
Оценка не на обучающих данных, а на тестовых (hold-out set)!
Проверка против бизнес-целей: Даже хорошая точность модели (Accuracy) может быть бесполезна, если она не решает изначальную проблему.
Решение: Запустить ли модель в работу или вернуться на предыдущие этапы.
Deployment:
Не просто модель, а рабочий процесс: Модель должна быть обернута в API, интегрирована в приложение или автоматизированный отчет.
Мониторинг: Дрейф данных (Data Drift) — со временем реальные данные могут меняться, и модель будет деградировать. Нужен план ее переобучения.
Вывод по теме 1: Анализ данных — это структурированный, итеративный процесс преобразования сырых данных в знания и решения. Его суть заключается не в слепом применении инструментов, а в решении конкретной бизнес- или исследовательской проблемы. Модель CRISP-DM служит универсальной картой этого процесса, подчеркивая, что понимание задачи и подготовка данных являются критически важными этапами, на которые приходится львиная доля времени и усилий. Успешный анализ всегда начинается с правильного вопроса.



Тема 2: Основы Python для анализа 
1. Философия Python для анализа: «Batteries included, but specialized tools rule»
В Python много встроенного, но для анализа мы используем специализированные «тяжелые» библиотеки (pandas, numpy), которые написанны на C/C++ для скорости.
2. Jupyter Notebook/Lab — ваша цифровая лаборатория
Ячейки (Cells) и их типы:
Code: Исполняемый код на Python.
Markdown: Текстовые пояснения, заголовки, формулы (LaTeX). Критически важно для документирования хода мысли!
Состояние ядра (Kernel): Хранит в памяти все переменные после выполнения ячеек. Позволяет работать интерактивно. Проблема: Можно запутаться в состоянии. Решение — периодически перезапускать ядро (Kernel -> Restart & Run All).
Магия (Magic commands): Команды, начинающиеся с % или %%. Например:
%timeit my_function() — замер времени выполнения.
%matplotlib inline — отображение графиков прямо в ноутбуке.
%%writefile script.py — запись ячейки в файл.
3. Критически важные концепции Python для аналитика
Динамическая типизация: Тип переменной (int, str, list) определяется в момент присвоения значения, а не при объявлении.
Ссылочная модель: Переменная — это «имя» (ссылка) на объект в памяти.
python
a = [1, 2, 3]
b = a  # b теперь указывает на ТОТ ЖЕ список, что и a
b.append(4)
print(a)  # [1, 2, 3, 4] !!! Изменение по ссылке
Чтобы создать копию, нужно явно использовать .copy() или list(a).
Импорт библиотек и алиасы (соглашения):
python
import numpy as np          # Стандартный алиас для NumPy
import pandas as pd         # Стандартный алиас для Pandas
import matplotlib.pyplot as plt # Стандартный алиас для Pyplot
Это экономит время и делает код общепринятым.
4. Структура типичного аналитического проекта
text
my_analysis_project/
├── data/                 # Папка с данными (сырыми и обработанными)
│   ├── raw/             # Исходные, неприкасаемые данные
│   └── processed/       # Очищенные данные
├── notebooks/           # Jupyter-ноутбуки с исследованием
│   └── 01_eda.ipynb
├── src/                 # Исходный код Python-модулей (функции, классы)
│   └── data_preprocessing.py
├── reports/             # Готовые отчеты, презентации, дашборды
├── requirements.txt     # Список зависимостей (библиотек)
└── README.md            # Описание проекта
Вывод по теме 2: Python стал главным языком анализа данных благодаря уникальному сочетанию простоты, читаемости и мощнейшей экосистемы специализированных библиотек. Интерактивная среда Jupyter Notebook является идеальным инструментом для исследований, позволяя смешивать код, визуализации и текстовые пояснения. Ключ к профессиональной работе — умение организовывать проекты и управлять зависимостями через виртуальные окружения, что обеспечивает воспроизводимость и стабильность результатов.


Тема 3: Типы данных и коллекции
1. Глубже в неизменяемость (Immutability)
Зачем это нужно? Безопасность (защита от случайного изменения), возможность использовать объект как ключ словаря, хешируемость.
Пример: tuple идеален для хранения координат (x, y) или записи о человеке (id, name, birthday), которую логически менять не должны.
frozenset: Неизменяемая версия set. Можно использовать как ключ в словаре.
2. Списки (list) — ваш основной «рабочий конь»
Методы, которые меняют список на месте (in-place):
.append(x) — добавить в конец (O(1)).
.extend(iterable) — добавить элементы итерируемого объекта (как +, но эффективнее).
.insert(i, x) — вставить на позицию i (O(n), может быть медленно).
.remove(x) — удалить первый элемент со значением x.
.pop([i]) — удалить и вернуть элемент по индексу (по умолчанию последний).
.sort() — сортировка на месте. Ключевой аргумент: key=func (например, key=len для сортировки по длине строк).
.reverse() — развернуть список на месте.
Методы, создающие новый объект:
sorted(list) — возвращает новый отсортированный список, оригинал не трогает.
reversed(list) — возвращает итератор (часто оборачивают в list()).
Генераторы списков (List Comprehension): Элегантный и быстрый способ создания списков.
python
# Вместо этого:
squares = []
for x in range(10):
    squares.append(x**2)
# Пишем так:
squares = [x**2 for x in range(10)]
# С условием:
even_squares = [x**2 for x in range(10) if x % 2 == 0]
3. Словари (dict) — основа для быстрого поиска и агрегации
Ключом может быть только хешируемый (hashable) объект (неизменяемый: int, float, str, tuple).
Методы для безопасной работы:
.get(key[, default]) — вернуть значение по ключу, если ключа нет — вернуть default (по умолчанию None). Не вызывает ошибку KeyError.
.setdefault(key[, default]) — если ключ есть, вернуть его значение. Если нет — вставить ключ со значением default и вернуть default.
.update(other_dict) — обновить словарь парами из другого словаря.
Словари для агрегации (паттерн):
python
# Подсчет частоты элементов (слов в тексте, городов в списке)
counts = {}
for item in item_list:
    counts[item] = counts.get(item, 0) + 1
4. Множества (set) — уникальность и теория множеств
Основные операции:
a | b (объединение) — элементы, которые в a, или в b, или в обоих.
a & b (пересечение) — элементы, которые и в a, и в b.
a - b (разность) — элементы, которые в a, но не в b.
a ^ b (симметрическая разность) — элементы, которые в a или в b, но не в обоих.
Практическое применение: Быстрая проверка на вхождение (x in set — O(1)), удаление дубликатов из списка (list(set(my_list))), нахождение общих элементов двух списков.
Вывод по теме 3: Понимание встроенных структур данных Python — фундамент для эффективной работы. Списки (list) и словари (dict) выступают основными "рабочими лошадками" для хранения и манипуляций данными в чистом Python, в то время как кортежи (tuple) обеспечивают безопасность неизменяемости, а множества (set) — уникальность и быстрые логические операции. Глубокое понимание их свойств (изменяемость, методы, принципы работы) позволяет писать не только корректный, но и производительный и понятный код.



Тема 4: NumPy 
1. Массив (ndarray) — фундаментальная структура
Однородность (Homogeneity): Все элементы одного типа (dtype). Это позволяет хранить данные плотно в памяти и эффективно применять операции.
dtype — мощный инструмент управления памятью и точностью:
np.int8, np.int16, np.int32, np.int64 — целые числа разной емкости.
np.float16, np.float32, np.float64 — числа с плавающей точкой разной точности.
np.bool_ — булевы значения.
np.object_ — массив Python-объектов (теряет многие преимущества NumPy, используется редко).
Можно указывать при создании: np.array([1,2,3], dtype=np.float32).
2. Индексация: базовая, булева (маски) и "причудливая" (fancy)
Булева индексация (Boolean Indexing) — один из самых мощных инструментов:
python
arr = np.array([1, 2, 3, 4, 5])
mask = arr > 2  # [False, False, True, True, True]
filtered = arr[mask]  # [3, 4, 5]
# Или в одну строку:
arr[arr % 2 == 0]  # Все четные: [2, 4]
"Причудливая" индексация (Fancy Indexing): Передача списка/массива индексов.
python
arr = np.array([10, 20, 30, 40, 50])
indices = [1, 3, 4]
arr[indices]  # [20, 40, 50]
3. Операции и оси (axis) — ключ к пониманию агрегации
Ось (axis) — это измерение массива. Для 2D массива:
axis=0 — ось строк (вертикальная, "вниз по столбцам").
axis=1 — ось столбцов (горизонтальная, "вдоль строк").
Агрегация по оси:
python
arr_2d = np.array([[1, 2, 3],
                   [4, 5, 6]])
np.sum(arr_2d, axis=0)  # Сумма по столбцам: [5, 7, 9] (сложили две строки)
np.mean(arr_2d, axis=1) # Среднее по строкам: [2., 5.] (усреднили элементы в каждой строке)
4. Трансляция (Broadcasting) — правила в деталях
Позволяет выполнять операции между массивами разной формы без их явного копирования.
Правила (по шагам):
Сравнить формы массивов справа налево.
Измерения считаются совместимыми, если:
Они равны, или
Одно из них равно 1.
Массив с меньшей размерностью мысленно "растягивается" по измерениям размера 1.
Если размерности не совместимы — ошибка.
Примеры:
(3, 4) + (4,) -> (3,4) + (1,4) -> (3,4) + (3,4) ✔
(2, 3) + (3,) -> (2,3) + (1,3) -> (2,3) + (2,3) ✔
(2, 3) + (2,) -> (2,3) + (1,2) -> (2,3) + (2,2) ❌ Ошибка! Последние размеры 3 и 2 не равны и ни одна не равна 1.
Практический пример:
python
# Вычитание среднего по столбцам из каждой строки (стандартизация)
data = np.array([[1, 2, 3],
                 [4, 5, 6]])
column_means = data.mean(axis=0)  # Форма (3,)
centered_data = data - column_means  # Broadcasting: (2,3) - (3,) -> работает!
5. Линейная алгебра "из коробки"
np.dot(a, b) или a @ b — матричное умножение.
np.linalg.inv(a) — обратная матрица.
np.linalg.eig(a) — собственные значения и векторы.
6. Производительность: Векторизация против циклов
Векторизованные операции — это операции, выполняемые над целыми массивами с помощью скомпилированного C-кода. Всегда стремитесь к ним.
Избегайте явных циклов for по элементам массивов NumPy. Они сводят на нет все преимущества библиотеки.
Вывод по теме 4: NumPy решает ключевую проблему Python для численных расчётов — производительность — через введение однородного многомерного массива ndarray. Векторизованные операции и трансляция (broadcasting) позволяют заменять неэффективные циклы на быстрые вычисления на уровне скомпилированного C-кода. NumPy не просто ускоряет вычисления, он задаёт идеологию работы с числовыми данными в экосистеме Python, выступая фундаментом для всех высокоуровневых библиотек, таких как Pandas и Scikit-learn.



Тема 5: Работа с табличными данными в Pandas
1. Философия и основа Pandas
Pandas — это высокоуровневая библиотека для работы с размеченными (labeled) и реляционными (табличными) данными. Построена на основе NumPy.
Ключевая идея: Предоставить интуитивно понятные структуры данных и операции для работы с таблицами, подобные тем, что есть в SQL или Excel, но с производительностью и мощью Python.
2. Две ключевые структуры данных
Series: Упрощённо — "умный" одномерный массив с индексом (labels). Это столбец таблицы с его собственными метками строк.
index — метки строк (не только целые числа 0,1,2..., но и строки, даты).
values — массив NumPy с данными.
dtype — тип данных в столбце.
DataFrame: Основная структура. Двумерная таблица с индексом строк и метками столбцов.
Это словарь из Series (столбцов), где ключ — название столбца.
Основа для 90% работы: Все операции очистки, анализа, агрегации происходят с DataFrame.
3. Создание DataFrame: основные способы
Из словаря списков/массивов (ключи → названия столбцов).
Из списка словарей (каждый словарь → строка).
Наиболее важный: Чтение из внешних файлов:
pd.read_csv() — из CSV (основной формат).
pd.read_excel() — из Excel.
pd.read_sql() — из базы данных SQL.
Ключевые параметры: sep, encoding, parse_dates, index_col.
4. Инспекция и базовое исследование данных
Форма и размер: .shape, .size.
Общая информация: .info() — типы данных, количество не-NULL значений в каждом столбце. Первый обязательный вызов.
Просмотр данных: .head(), .tail(), .sample().
Основные статистики: .describe() — считает статистики только для числовых столбцов. Для категориальных — используйте .describe(include='object').
Проверка дубликатов и уникальности: .duplicated(), .drop_duplicates(), .nunique(), .unique().
5. Выбор данных (Indexing and Selection) — критически важный навык
Существует два основных интерфейса:
df[ ] (квадратные скобки):
Одна скобка: df['col_name'] → вернёт Series. df[['col1', 'col2']] → вернёт DataFrame.
Срез: df[5:10] → срез по строкам (только по целочисленным позициям).
.loc[] — выбор по МЕТКАМ (labels):
df.loc[10] → строка с индексом 10 (как df.iloc[10] если индекс числовой).
df.loc[5:10, 'col_name'] → строки с метками от 5 до 10 включительно и столбец 'col_name'.
df.loc[df['age'] > 25, ['name', 'city']] → булева индексация по строкам + выбор столбцов.
.iloc[] — выбор по ЦЕЛОЧИСЛЕННЫМ ПОЗИЦИЯМ (integer-location):
df.iloc[5] → строка на позиции 5 (0-based).
df.iloc[5:10, 2:4] → строки с 5 по 9 и столбцы с 2 по 3 (как в срезах Python).
Важно: .iloc[5:10] не включает 10-ю позицию.
Главное правило: .loc — для меток, .iloc — для позиций. Смешивать нельзя.
6. Базовые операции с DataFrame
Добавление/удаление столбцов:
df['new_col'] = values — добавление.
df.drop(columns=['col1', 'col2'], inplace=True) — удаление. inplace=True меняет исходный df.
Переименование: df.rename(columns={'old':'new'}, inplace=True).
Сортировка: df.sort_values(by='col', ascending=False).
Транспонирование: df.T.
7. Группировка и агрегация (основы)
df.groupby('key_column') — разбивает DataFrame на группы по уникальным значениям указанного столбца.
К группировке можно применять агрегирующие функции:
.sum(), .mean(), .count(), .min(), .max(), .std().
.agg({'col1':'sum', 'col2':['mean', 'std']}) — продвинутая агрегация для разных столбцов.
Результат группировки — специальный объект, который можно преобразовать обратно в DataFrame.
Вывод по теме 5: Pandas — это основной инструмент для загрузки, хранения и первичного манипулирования табличными данными. Освоение DataFrame, методов инспекции .info()/.describe() и корректного выбора данных (.loc/.iloc) — фундамент для всего последующего анализа.

Тема 6: Операции очистки и предобработки данных
1. Цель этапа
Преобразовать "сырые" данные в пригодный для анализа и моделирования формат. Решает проблемы: неконсистентность, беспорядок, некорректные типы данных.
2. Работа со столбцами (Features/Columns)
Приведение названий столбцов к единому стандарту:
Удаление пробелов: df.columns = df.columns.str.strip().
Приведение к нижнему регистру: df.columns = df.columns.str.lower().
Замена проблемных символов: df.columns = df.columns.str.replace(' ', '_').
Выбор и перестановка столбцов: df = df[['col1', 'col3', 'col2']] — создает новый DataFrame в нужном порядке.
3. Работа со строками (Observations/Rows)
Фильтрация данных по условию: Использование булевых масок.
df = df[df['age'] >= 18] — оставить только совершеннолетних.
df = df[(df['dept'] == 'Sales') & (df['salary'] > 50000)] — комбинирование условий.
Удаление дубликатов:
df.duplicated(subset=['col1', 'col2']) — поиск дубликатов по комбинации столбцов.
df.drop_duplicates(subset=['col1', 'col2'], keep='first', inplace=True) — удаление. keep='last' оставит последнюю копию.
4. Преобразование типов данных (Type Casting)
Проблема: Числа могут быть прочитаны как строки (объекты), даты — как строки.
Методы:
df['col'] = df['col'].astype('int64') — явное преобразование.
pd.to_numeric(df['col'], errors='coerce') — умное преобразование в число. errors='coerce' превратит ошибки в NaN.
pd.to_datetime(df['date_col'], format='%d.%m.%Y', errors='coerce') — преобразование в дату. Всегда указывайте format, если известен.
df['col'] = df['col'].astype('category') — для ограниченного набора строковых значений (экономия памяти, повышение скорости).
5. Обработка строковых данных (String Operations)
Доступ к строковым методам через .str:
df['name'].str.upper() / .str.lower() / .str.title().
df['email'].str.contains('gmail.com') — поиск подстроки.
df['phone'].str.replace('-', '') — замена символов.
df['full_name'].str.split(' ', expand=True) — разделение строки на несколько столбцов.
Обрезка пробелов: df['col'] = df['col'].str.strip().
6. Создание новых признаков (Feature Engineering - основы)
Извлечение частей из строк: Год из даты (df['year'] = df['date'].dt.year), домен из email.
Биннинг (Binning): Превращение непрерывного признака (возраст) в категориальный (возрастные группы).
pd.cut(df['age'], bins=[0, 18, 65, 100], labels=['child', 'adult', 'senior']).
Применение функций: df['new'] = df['col'].apply(lambda x: x*2) или df['new'] = df.apply(lambda row: row['a']+row['b'], axis=1).
Вывод по теме 6: Очистка — это последовательность рутинных, но жизненно важных операций: привидение столбцов в порядок, фильтрация строк, корректное задание типов и работа со строками. Качественно очищенные данные — залог успеха любого анализа.

Тема 7: Работа с пропущенными значениями и выбросами
1. Пропущенные значения (Missing Values)
Обозначения: NaN (Not a Number, float), None (объект Python), NaT (Not a Time, для datetime).
Обнаружение:
df.isna() или df.isnull() — булева матрица.
df.isna().sum() — количество пропусков в каждом столбце.
df.isna().sum().sum() — общее количество пропусков.
2. Причины появления пропусков и стратегии обработки
MCAR (Missing Completely at Random): Пропуск ни с чем не связан. Редко. Можно удалять без сильного смещения.
MAR (Missing at Random): Пропуск связан с другими наблюдаемыми переменными. Нужна аккуратная обработка.
MNAR (Missing Not at Random): Пропуск связан с самим пропущенным значением (например, богатые люди отказываются указывать доход). Самая проблемная ситуация.
3. Методы обработки пропусков
Удаление:
df.dropna(axis=0) — удаление строк, содержащих хотя бы один NaN.
df.dropna(axis=1, thresh=0.7*len(df)) — удаление столбцов, где более 30% значений пропущены.
Минус: Потеря информации, может привести к смещенной выборке.
Заполнение (Imputation):
Константой: df.fillna(0) или df.fillna('Unknown').
Статистикой: df.fillna(df['col'].mean()) или .median(), .mode()[0].
Для категориальных данных — мода.
Для числовых — медиана (устойчива к выбросам) или среднее.
Продвинутые методы: Заполнение на основе других признаков, использование моделей (KNN, регрессия), интерполяция для временных рядов (df.interpolate()).
Важно: Заполненные значения искажают распределение и дисперсию признака. Это нужно учитывать при моделировании.
4. Выбросы (Outliers) — аномальные значения
Причины: Ошибка измерения, редкое событие, естественная изменчивость.
Обнаружение:
Визуальное: Box-plot (ящик с усами), гистограмма, scatter plot.
Статистическое:
Правило 3-х сигм (для нормального распределения): Выбросы за пределами mean ± 3*std.
Метод межквартильного размаха (IQR, универсальный):
Q1 = df['col'].quantile(0.25)
Q3 = df['col'].quantile(0.75)
IQR = Q3 - Q1
Нижняя граница: Q1 - 1.5 * IQR
Верхняя граница: Q3 + 1.5 * IQR
Значения за этими границами считаются выбросами.
5. Методы обработки выбросов
Удаление: df = df[(df['col'] >= lower_bound) & (df['col'] <= upper_bound)].
Каппирование (Winsorizing): Замена выбросов на граничные значения (например, все значения выше 99-го перцентиля приравниваются к значению 99-го перцентиля).
python
cap = df['col'].quantile(0.99)
df['col_capped'] = df['col'].clip(upper=cap)
Преобразование: Логарифмирование (np.log1p) может "прижать" очень большие значения.
Создание отдельной категории: Для моделей, основанных на деревьях, выброс можно выделить в отдельную метку (например, ">100k").
Вывод по теме 7: Пропуски и выбросы искажают статистики и работу моделей. Нет единственно правильного способа обработки. Выбор стратегии зависит от природы данных, доли аномалий и цели анализа. Всегда документируйте, что и как вы делали.

Тема 8: Визуализация данных: основные библиотеки и подходы
1. Цель визуализации
Для аналитика (EDA): Увидеть паттерны, аномалии, распределения, связи. Визуализация — это инструмент мышления.
Для презентации: Донести идею, вывод или историю до аудитории. Визуализация — это инструмент убеждения.
2. Библиотеки и их экосистема
Matplotlib: Фундаментальная, низкоуровневая библиотека. Даёт полный контроль над каждым элементом графика (как "рисование карандашом"). Часто используется как "движок" для других библиотек.
pyplot интерфейс (plt): Упрощённый, stateful-интерфейс.
Объектно-ориентированный интерфейс (OO): fig, ax = plt.subplots() — более гибкий и рекомендуемый для сложных графиков.
Seaborn: Высокоуровневая библиотека на основе Matplotlib. Создана для статистической графики. Красивые стили по умолчанию, удобный API для сложных графиков (например, pairplot, heatmap).
Pandas Plotting: Простейшие графики прямо из Series/DataFrame (df.plot(), df['col'].hist()). Основан на Matplotlib, хорош для быстрого просмотра.
3. Основные типы графиков и их назначение
Для одной числовой переменной (распределение):
Гистограмма (hist): Показывает частоту значений в интервалах (бинах). Позволяет оценить форму, центр, разброс, модальность, асимметрию.
Ящик с усами (Box plot, boxplot): Показывает медиану, квартили (IQR) и выбросы. Отлично подходит для сравнения распределений между группами.
Плотность распределения (KDE, kdeplot): Сглаженная оценка функции плотности.
Для двух переменных (связи):
Точечный график (Scatter plot, scatter): Связь между двумя непрерывными переменными. Показывает тренд, силу связи, кластеры.
Линейный график (Line plot, plot): Для временных рядов (изменение во времени).
Для категориальных данных:
Столбчатая диаграмма (Bar chart, bar): Сравнение величин по категориям (например, средний доход по городам).
Гистограмма частот (Count plot, countplot в Seaborn): Показывает количество наблюдений в каждой категории (распределение категориального признака).
Для многомерных данных:
Матрица диаграмм рассеяния (Scatterplot Matrix, pairplot в Seaborn): Позволяет увидеть попарные связи между многими числовыми признаками.
Тепловая карта (Heatmap, heatmap): Отлично подходит для визуализации матрицы корреляций.
4. Принципы эффективной визуализации
Избегайте chartjunk (визуального мусора): Лишние сетки, 3D-эффекты для 2D-данных, неинформативные подписи.
Выбирайте правильный тип графика под задачу (сравнение, распределение, связь, часть от целого).
Работа с осями:
Всегда подписывайте оси и давайте заголовок.
Начинайте ось Y с 0 для столбчатых диаграмм (bar charts), иначе визуально искажаете разницу.
Используйте логарифмическую шкалу (ax.set_yscale('log')), если данные различаются на порядки.
Использование цвета:
Цвет — мощный инструмент для выделения категорий.
Для непрерывных данных используйте последовательные палитры (sequential colormaps).
Для категориальных — качественные палитры (qualitative colormaps).
Учитывайте дальтонизм (избегайте красно-зелёных комбинаций).
5. Типичный рабочий процесс в Jupyter
python
import matplotlib.pyplot as plt
import seaborn as sns
# 1. Установка стиля (опционально)
sns.set_style("whitegrid")
# 2. Создание фигуры и осей
fig, ax = plt.subplots(figsize=(10, 6)) # figsize в дюймах
# 3. Построение графика (например, через Seaborn)
sns.histplot(data=df, x='age', kde=True, ax=ax)
# 4. Кастомизация
ax.set_title('Распределение возраста', fontsize=16)
ax.set_xlabel('Возраст (лет)')
ax.set_ylabel('Количество')
# 5. Отображение
plt.tight_layout() # Автоматическая подгонка отступов
plt.show()
Вывод по теме 8: Визуализация — не просто "красивые картинки", а основной язык исследования данных. Умение выбрать правильный тип графика, построить его с помощью Matplotlib/Seaborn и корректно его подписать — ключевой навык аналитика как на этапе исследования (EDA), так и на этапе презентации результатов.
Тема 9: Исследовательский анализ данных (EDA)
1. Философия и цели EDA
EDA (Exploratory Data Analysis) — это неформальный, интерактивный и творческий процесс изучения данных до построения формальных моделей или проверки гипотез.
Главная цель: Понять, "о чём говорят данные", выявить скрытые паттерны, аномалии, взаимосвязи и сформировать гипотезы. EDA — это "разговор" с данными.
Ключевые принципы:
Визуализация — первична: Графики часто показывают то, что не видят статистики.
Гибкость: Нет одного правильного пути. Методы меняются в зависимости от вопросов и природы данных.
Итеративность: Каждый вопрос или находка порождает новые вопросы.
2. Структура и этапы EDA (системный подход)
Формирование понимания данных (Data Understanding):
Загрузка, первичный осмотр (shape, columns).
Изучение метаинформации: что означает каждый признак (Data Dictionary).
Проверка типов данных (dtypes) и их коррекция.
Оценка качества данных (Data Quality Assessment):
Поиск пропусков (isna().sum()), дубликатов, неконсистентных значений (опечатки в категориальных признаках).
Выявление явных выбросов на этом этапе.
Анализ распределений (Univariate Analysis — анализ одной переменной):
Для числовых признаков: Гистограмма + boxplot + описательные статистики (describe()). Оценка центра (mean, median), разброса (std, IQR), формы (skewness, modality).
Для категориальных признаков: Bar chart / countplot + таблица частот (value_counts()). Оценка дисбаланса классов.
Анализ взаимосвязей (Bivariate/Multivariate Analysis):
Числовой vs Числовой: Scatter plot (для 2 признаков) + матрица корреляций + heatmap (для многих признаков).
Категориальный vs Числовой: Boxplot (или violinplot) для сравнения распределений числовой величины по категориям. Использование агрегаций (groupby().mean()).
Категориальный vs Категориальный: Сводная таблица (pivot table) с подсчётом или долями, heatmap, stacked bar chart.
Формирование и документирование инсайтов:
Ответы на вопросы: "Что я узнал?", "Какие гипотезы можно сформулировать?", "Какие данные нужно очистить/преобразовать для моделирования?".
3. Инструменты и техники для углублённого EDA
pandas_profiling / ydata_profiling: Автоматическое создание развёрнутого отчёта (распределения, корреляции, пропуски, взаимодействия). Отличная точка входа.
Сводные таблицы (Pivot Tables): pd.pivot_table(df, index='cat_col', values='num_col', aggfunc=['mean', 'count']). Мощный инструмент для многомерного анализа.
Парные графики (PairPlot): sns.pairplot(df, hue='target') — позволяет сразу увидеть взаимосвязи и различия по целевой переменной.
Матрица корреляций с аннотацией:
python
corr_matrix = df.corr(numeric_only=True)
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', center=0)
Исследование целевой переменной (Target Variable Analysis): Самое важное. Как распределена целевая переменная? Сбалансированы ли классы? Как другие признаки с ней связаны?
4. Типичные вопросы в ходе EDA
Есть ли сезонность или тренд во временных данных?
Есть ли явные кластеры в данных на scatter plot?
Есть ли признаки мультиколлинеарности (сильной корреляции между предикторами)?
Насколько признаки связаны с целевой переменной?
Какие признаки имеют потенциал для feature engineering?
Вывод по теме 9: EDA — это основа основ аналитики. Это процесс, где вы становитесь "знакомы" со своим датасетом. Без качественного EDA последующее моделирование строится на зыбком фундаменте. Цель — не просто построить графики, а задавать правильные вопросы и находить истории в данных.

Тема 10: Работа с временными рядами в Pandas
1. Что такое временной ряд?
Временной ряд (Time Series) — это последовательность данных, упорядоченных во времени, где время является ключевой независимой переменной.
Особенности: Наличие тренда, сезонности, цикличности и шума. Наблюдения часто автокоррелированы (значение в момент t зависит от значений в t-1, t-2...).
2. Поддержка временных рядов в Pandas
Специальный dtype: datetime64[ns].
Индекс DatetimeIndex: Когда индекс DataFrame является datetime, открывается доступ к специальным методам ресемплинга и сдвига.
Создание: pd.to_datetime(), pd.date_range().
Важно: При чтении данных указывать столбец с датой как индекс: df = pd.read_csv('data.csv', parse_dates=['date_col'], index_col='date_col').
3. Базовые операции с временными индексами
Частичный доступ (Partial String Indexing): Можно срезать данные по году, месяцу и т.д.
python
df['2023']        # Все данные за 2023 год
df['2023-05']     # Все данные за май 2023
df['2023-05-10':'2023-05-15']  # Срез по датам
Извлечение атрибутов времени:
python
df.index.year
df.index.month
df.index.dayofweek  # День недели (понедельник=0)
df.index.quarter
Сдвиг данных (Shifting / Lagging): df['value_lag1'] = df['value'].shift(periods=1) — получение значений предыдущего периода (лагов). Критично для прогнозных моделей.
Разность (Differencing): df['value_diff'] = df['value'].diff(periods=1) — вычисление разности с предыдущим значением. Используется для устранения тренда.
4. Ресемплинг (Resampling) и агрегация по времени
Ресемплинг — это процесс изменения частоты временного ряда (например, с ежедневных данных на еженедельные).
Понижающая дискретизация (Downsampling): Переход к более низкой частоте (дни → месяцы). Требует агрегации.
python
# Среднее значение за месяц
df.resample('M').mean()
# Сумма за неделю (понедельник - воскресенье)
df.resample('W').sum()
Повышающая дискретизация (Upsampling): Переход к более высокой частоте (часы → минуты). Требует интерполяции или заполнения пропусков.
python
df.resample('D').asfreq()          # Появится NaN для новых дат
df.resample('D').ffill() / .bfill() # Заполнение вперед/назад
df.resample('D').interpolate()      # Линейная интерполяция
Скользящие (подвижные) окна (Rolling Windows): Вычисление статистик для "скользящего" окна фиксированного размера (полезно для сглаживания).
python
df['rolling_mean_7'] = df['value'].rolling(window=7).mean()  # 7-дневное скользящее среднее
df['rolling_std_30'] = df['value'].rolling(window=30).std()  # 30-дневное скользящее СКО
Расширяющиеся окна (Expanding Windows): Окно увеличивается от начала ряда до текущей точки (кумулятивные статистики).
python
df['expanding_mean'] = df['value'].expanding().mean()
5. Визуализация временных рядов
Базовый line plot: df['value'].plot(figsize=(12,6)).
Несколько рядов на одном графике для сравнения.
Визуализация компонент: Разложение ряда на тренд, сезонность и остаток (можно с помощью statsmodels).
Графики скользящей статистики для выявления тренда на зашумленных данных.
6. Поиск тренда и сезонности
Тренд: Долгосрочное направленное движение данных (рост, спад, стагнация). Выявляется скользящим средним или линейной регрессией по времени.
Сезонность: Периодические колебания с фиксированной частотой (сутки, неделя, год). Выявляется разложением ряда или анализом автокорреляционной функции.
Вывод по теме 10: Работа с временными рядами требует особого подхода из-за присущей им структуры и зависимостей. Pandas предоставляет мощный и удобный инструментарий для манипуляций с временными данными: от базового парсинга дат до сложного ресемплинга и вычисления скользящих статистик, которые являются основой для дальнейшего анализа и прогнозирования.

Тема 11: Основы статистики для анализа данных
1. Зачем аналитику статистика?
Чтобы делать обоснованные выводы из данных, а не строить догадки.
Чтобы корректно интерпретировать результаты моделей машинного обучения.
Чтобы понимать, являются ли обнаруженные паттерны статистически значимыми или они возникли случайно.
2. Описательная статистика (Descriptive Statistics)
Меры центральной тенденции (Central Tendency):
Среднее (Mean): Сумма всех значений, делённая на их количество. Чувствительно к выбросам.
Медиана (Median): Значение, которое делит упорядоченный набор пополам. Устойчивая к выбросам мера.
Мода (Mode): Наиболее часто встречающееся значение.
Меры изменчивости (Variability / Spread):
Размах (Range): Разность между максимальным и минимальным значениями. Очень чувствителен к выбросам.
Дисперсия (Variance): Среднее арифметическое квадратов отклонений от среднего. Var(X) = Σ(x_i - mean)² / (n-1) (несмещённая оценка).
Стандартное отклонение (Standard Deviation, SD, σ): Квадратный корень из дисперсии. Измеряется в тех же единицах, что и данные. Показывает "типичное" отклонение от среднего.
Межквартильный размах (Interquartile Range, IQR): IQR = Q3 - Q1. Устойчив к выбросам. Основа для boxplot.
Меры формы распределения (Shape):
Асимметрия (Skewness): Показывает, насколько распределение несимметрично.
skew > 0 — правый хвост длиннее (положительная асимметрия, мода < медиана < среднее).
skew < 0 — левый хвост длиннее (отрицательная асимметрия).
Эксцесс (Kurtosis): Показывает "остроту" пика и тяжесть хвостов по сравнению с нормальным распределением.
kurtosis > 0 — тяжёлые хвосты (распределение имеет больше выбросов, чем нормальное).
kurtosis < 0 — легкие хвосты.
3. Основы теории вероятностей для анализа
Распределения вероятностей:
Нормальное (Гауссово) распределение: Колоколообразная кривая. Многие статистические методы предполагают нормальность данных. Определяется средним (μ) и стандартным отклонением (σ).
Равномерное распределение: Все значения равновероятны.
Биномиальное распределение: Число "успехов" в серии независимых испытаний.
Центральная предельная теорема (ЦПТ): При достаточно большом объёме выборки (n > 30) выборочное распределение среднего будет стремиться к нормальному распределению независимо от формы исходного распределения данных. Фундамент для статистического вывода.
4. Статистический вывод (Statistical Inference)
Генеральная совокупность (Population) и Выборка (Sample): Мы почти всегда работаем с выборкой, чтобы делать выводы о популяции.
Выборочная статистика (Sample Statistic, например, x̄ — выборочное среднее) — это оценка параметра популяции (Population Parameter, μ — истинное среднее).
Распределение выборочного среднего: В соответствии с ЦПТ, оно нормально со средним μ и стандартным отклонением σ / √n (стандартная ошибка среднего, SEM).
5. Интервальное оценивание (Доверительные интервалы)
Доверительный интервал (Confidence Interval, CI) — это диапазон значений, который с заданной вероятностью (уровнем доверия, например, 95%) содержит истинный параметр популяции.
Формула для среднего (при известном σ или большом n): x̄ ± Z*(σ/√n), где Z — Z-оценка, соответствующая уровню доверия (для 95% это ~1.96).
Интерпретация: Если мы построим 100 таких 95% доверительных интервалов по разным выборкам из одной популяции, то примерно 95 из них будут содержать истинное среднее μ.
6. Ключевые концепции проверки гипотез (введение)
Нулевая гипотеза (H₀): Консервативное предположение, которое мы пытаемся опровергнуть (например, "лекарство неэффективно", "средние двух групп равны").
Альтернативная гипотеза (H₁ или Hₐ): Утверждение, которое мы хотим доказать ("лекарство эффективно", "средние различаются").
p-value (p-уровень значимости): Вероятность получить наблюдаемые или ещё более крайние результаты при условии, что нулевая гипотеза верна.
Малое p-value (< α, где α — уровень значимости, обычно 0.05) говорит о том, что наблюдаемые данные маловероятны, если H₀ верна. Это дает основание отвергнуть H₀.
Большое p-value: Нет достаточных оснований отвергнуть H₀.
Ошибки:
Ошибка I рода (False Positive): Отвергнуть верную H₀. Вероятность = α.
Ошибка II рода (False Negative): Не отвергнуть ложную H₀. Вероятность = β.
Вывод по теме 11: Статистика — это язык, на котором говорит анализ данных. Понимание описательных статистик позволяет резюмировать данные, а знание основ статистического вывода (ЦПТ, доверительные интервалы, p-value) позволяет корректно переносить выводы с выборки на всю генеральную совокупность и принимать обоснованные решения.

Тема 12: Корреляция и ковариация. Проверка гипотез
1. Ковариация (Covariance)
Ковариация измеряет направление линейной связи между двумя переменными.
Формула: Cov(X,Y) = Σ[(x_i - x̄)(y_i - ȳ)] / (n-1)
Интерпретация знака:
Cov > 0 — Прямая связь (с ростом X растёт Y).
Cov < 0 — Обратная связь (с ростом X уменьшается Y).
Cov ≈ 0 — Линейная связь отсутствует.
Недостаток: Величина ковариации зависит от единиц измерения переменных. Нельзя сказать, сильная связь или слабая.
2. Корреляция Пирсона (Pearson Correlation Coefficient)
Коэффициент корреляции Пирсона (r) — это стандартизированная ковариация. Не имеет размерности.
Формула: r = Cov(X,Y) / (σ_x * σ_y)
Свойства:
-1 ≤ r ≤ 1
r = 1 — Идеальная прямая линейная связь.
r = -1 — Идеальная обратная линейная связь.
r = 0 — Отсутствие линейной связи (но может быть нелинейная!).
Интерпретация величины (примерная):
|r| < 0.3 — слабая связь.
0.3 ≤ |r| < 0.7 — умеренная связь.
|r| ≥ 0.7 — сильная связь.
Важное ограничение: Корреляция измеряет только линейную связь. Она не улавливает сложные нелинейные зависимости.
3. Проверка значимости корреляции
Наблюдаемый коэффициент корреляции r вычислен по выборке. Нужно проверить, значимо ли он отличается от нуля в популяции.
Нулевая гипотеза H₀: ρ = 0 (истинная корреляция в популяции равна нулю).
Альтернативная гипотеза H₁: ρ ≠ 0 (корреляция в популяции существует).
Используется t-тест для корреляции: t = r * √(n-2) / √(1 - r²).
По значению t и объёму выборки вычисляется p-value.
Вывод: Если p-value < α (например, 0.05), то отвергаем H₀ и делаем вывод, что корреляция статистически значима.
4. Общая логика проверки статистических гипотез (расширение)
Формулировка гипотез: H₀ и H₁.
Выбор уровня значимости (α): Вероятность совершить ошибку I рода (обычно 0.05 или 0.01).
Выбор статистического критерия (теста) в зависимости от типа данных и задачи:
t-критерий Стьюдента: Сравнение средних (1 выборка с константой, 2 независимые выборки, 2 зависимые выборки).
Критерий Хи-квадрат (χ²): Проверка связи между категориальными переменными или соответствия распределения.
Критерий Манна-Уитни (U-тест): Непараметрический аналог t-теста для 2 независимых выборок (не требует нормальности).
ANOVA: Сравнение средних в трёх и более группах.
Расчёт наблюдаемого значения статистики теста по выборочным данным.
Определение p-value: Вероятность получить такое или более крайнее значение статистики при условии истинности H₀.
Принятие решения:
p-value ≤ α → Отвергаем H₀ в пользу H₁. Результат статистически значим.
p-value > α → Не отвергаем H₀. Статистически значимых доказательств в пользу H₁ нет.
5. Практические аспекты и предостережения
Корреляция ≠ Причинно-следственная связь: Сильная корреляция не означает, что одна переменная вызывает изменения в другой. Возможны:
Случайность.
Влияние третьей, скрытой переменной (confounding variable).
Обратная причинность.
Проблема множественного тестирования: Если провести много статистических тестов, возрастает вероятность случайно получить значимый результат (ошибка I рода). Нужны поправки (например, Бонферрони).
Статистическая значимость vs Практическая значимость: Очень слабый эффект (например, r=0.1) может быть статистически значимым на огромной выборке, но абсолютно бесполезным на практике. Всегда смотрите на величину эффекта (например, сам коэффициент r).
Вывод по теме 12: Корреляция — это первый шаг к анализу взаимосвязей, но её интерпретация требует осторожности. Проверка гипотез — это формальный механизм, позволяющий отделить систематические эффекты от случайного шума в данных, но его результаты нужно интерпретировать в контексте предметной области и размера эффекта.
Тема 13: Основы машинного обучения: постановка задачи и подготовка данных
1. Что такое машинное обучение (ML)?
Классическое определение (Артур Самуэль): "Область исследований, которая дает компьютерам способность учиться без явного программирования".
Современное определение: Создание моделей (алгоритмов), которые на основе обучающих данных могут выявлять закономерности (паттерны) и использовать их для прогнозирования на новых данных или принятия решений.
2. Типы задач машинного обучения
Обучение с учителем (Supervised Learning): В данных есть целевая переменная (target, label), которую модель учится предсказывать.
Классификация (Classification): Целевая переменная — категория (класс). Примеры: спам/не спам, диагноз (A/B/C), вид ириса. Основные алгоритмы: логистическая регрессия, решающие деревья, метод k-ближайших соседей (KNN).
Регрессия (Regression): Целевая переменная — непрерывное число. Примеры: цена дома, температура завтра, время доставки. Основные алгоритмы: линейная регрессия, деревья регрессии.
Обучение без учителя (Unsupervised Learning): В данных нет целевой переменной. Задача — найти скрытую структуру в данных.
Кластеризация (Clustering): Разбиение данных на группы (кластеры) так, чтобы объекты внутри группы были похожи, а между группами — различны. Примеры: сегментация клиентов, группировка документов. Основной алгоритм: K-Means.
Снижение размерности (Dimensionality Reduction): Упрощение данных за счёт уменьшения количества признаков при сохранении максимального количества информации. Используется для визуализации или ускорения работы моделей. Основной метод: PCA.
Обучение с подкреплением (Reinforcement Learning): Агент учится, взаимодействуя со средой и получая награду за правильные действия. (В вашем курсе, скорее всего, не рассматривается детально).
3. Ключевые понятия и этапы ML-проекта
Признаки (Features): Входные переменные (столбцы в DataFrame, кроме целевого). Это "сырьё" для модели.
Целевая переменная (Target/Label): То, что мы предсказываем.
Модель (Model): Алгоритм + найденные в процессе обучения параметры.
Обучение (Training/Fitting): Процесс "подгонки" модели под обучающие данные (нахождение параметров, минимизирующих ошибку).
Прогноз (Prediction/Inference): Применение обученной модели к новым данным для получения ответа.
Этапы проекта:
Постановка задачи: Перевести бизнес-проблему ("хочу увеличить продажи") в задачу ML ("предсказать, какой клиент купит").
Сбор и подготовка данных (80% времени): EDA, очистка, feature engineering.
Разделение данных: X_train, X_test, y_train, y_test (часто также X_val, y_val).
Выбор и обучение модели: Подбор алгоритма, его обучение на train-выборке.
Оценка модели: Тестирование на test-выборке (которую модель не видела при обучении).
Настройка гиперпараметров: Поиск лучших настроек алгоритма на валидационной выборке.
Интерпретация и внедрение.
4. Подготовка данных для ML (Feature Engineering для моделей)
Кодирование категориальных признаков:
Label Encoding: Присвоение каждой категории числа (0, 1, 2...). Плохо для линейных моделей, если порядок не имеет смысла (например, "Москва", "Париж", "Токио").
One-Hot Encoding (OHE): Создание нового бинарного (0/1) признака для каждой категории. Используется для номинальных признаков (без порядка). pd.get_dummies().
Масштабирование числовых признаков: Критически важно для алгоритмов, использующих расстояние (KNN, SVM) или градиентный спуск (линейная/логистическая регрессия).
StandardScaler: Приводит данные к распределению с mean=0 и std=1. z = (x - u) / s. Лучший выбор по умолчанию.
MinMaxScaler: Приводит все значения к диапазону [0, 1]. X_scaled = (X - X.min) / (X.max - X.min). Чувствителен к выбросам.
Важно: Масштабировать только на train-выборке, а затем применять те же параметры (среднее/стандартное отклонение) к test-выборке.
5. Разделение данных и проблема "утечки данных" (Data Leakage)
Принцип: Модель не должна никоим образом "видеть" данные из тестовой выборки в процессе обучения.
Типичная ошибка: Масштабировать или заполнять пропуски по всему датасету до разделения. Правильно: разделить, затем все преобразования делать, обучаясь только на X_train.
Функция train_test_split: from sklearn.model_selection import train_test_split
python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
random_state — для воспроизводимости результатов.
stratify=y — для сохранения пропорций классов в разбиении (важно для несбалансированных данных).
Вывод по теме 13: Машинное обучение — это процесс превращения данных в предсказательные модели. Ключ к успеху — четкая постановка задачи, тщательная подготовка данных (кодирование, масштабирование) и строгое разделение данных на обучающую и тестовую выборки для честной оценки.

Тема 14: Линейная и логистическая регрессия
1. Линейная регрессия (Linear Regression)
Идея: Смоделировать целевую переменную y как линейную комбинацию признаков X плюс свободный член (intercept).
Математическая модель (для одного признака): y = w₁*x + w₀
w₀ (intercept) — смещение, точка пересечения с осью Y.
w₁ (coefficient) — вес (коэффициент) признака. Показывает, на сколько изменится y при изменении x на 1.
Многомерный случай: y = w₀ + w₁*x₁ + w₂*x₂ + ... + wₙ*xₙ
Обучение (как найти w?): Метод наименьших квадратов (Ordinary Least Squares, OLS). Ищем такие веса w, чтобы минимизировать функцию потерь (Loss Function) — сумму квадратов ошибок (разниц между предсказанием и истинным значением) по всем объектам обучающей выборки.
2. Оценка качества регрессии
Метрики (необходимо знать несколько):
MAE (Mean Absolute Error): Средняя абсолютная ошибка. MAE = (1/n) * Σ|y_i - ŷ_i|. Легко интерпретировать ("в среднем ошибаемся на ±X единиц").
MSE (Mean Squared Error): Среднеквадратичная ошибка. MSE = (1/n) * Σ(y_i - ŷ_i)². Сильнее штрафует за большие ошибки.
RMSE (Root Mean Squared Error): Корень из MSE. RMSE = sqrt(MSE). Измеряется в тех же единицах, что и y.
R² (коэффициент детерминации): Доля дисперсии целевой переменной, объясненная моделью. R² = 1 - (SS_res / SS_tot), где SS_res — сумма квадратов остатков, SS_tot — общая сумма квадратов.
R² = 1 — идеальная модель.
R² = 0 — модель не лучше, чем предсказание константой (средним значением).
R² может быть отрицательным на тестовых данных, если модель работает ужасно.
3. Логистическая регрессия (Logistic Regression)
Идея: Несмотря на название, это алгоритм для бинарной классификации (2 класса, например, 0 и 1).
Выход модели: Не прямое предсказание класса, а вероятность принадлежности к классу 1 (p = P(y=1|X)).
Математика: Применяет логистическую (сигмоидную) функцию к линейной комбинации признаков, чтобы "сжать" результат в интервал [0, 1].
p = 1 / (1 + e^-(w₀ + w₁*x₁ + ...))
Сигмоида превращает любое число в вероятность.
Принятие решения: Если p >= 0.5 (порог по умолчанию), предсказываем класс 1, иначе — 0. Порог можно менять в зависимости от задачи.
4. Оценка качества классификации
Матрица ошибок (Confusion Matrix): Основная таблица для анализа.
text
           | Предсказано 0 | Предсказано 1
------------|---------------|--------------
Истинно 0   | TN            | FP
Истинно 1   | FN            | TP
TP (True Positive): Верно предсказали "1".
TN (True Negative): Верно предсказали "0".
FP (False Positive): Ошибочно предсказали "1" (ошибка I рода).
FN (False Negative): Ошибочно предсказали "0" (ошибка II рода).
Метрики, производные от матрицы:
Accuracy (Доля верных ответов): (TP+TN) / (TP+TN+FP+FN). Не работает на несбалансированных данных.
Precision (Точность): TP / (TP+FP). Какая доля объектов, предсказанных как "1", действительно "1"? (Качество позитивного прогноза).
Recall (Полнота, Sensitivity): TP / (TP+FN). Какая доле реальных "1" была предсказана правильно? (Способность находить все позитивные случаи).
F1-score: Гармоническое среднее Precision и Recall. F1 = 2 * (Precision * Recall) / (Precision + Recall). Баланс между двумя метриками.
ROC-кривая и AUC-ROC:
ROC-кривая: Строится для разных порогов классификации. По оси X — FPR (False Positive Rate = FP/(FP+TN)), по оси Y — TPR (True Positive Rate = Recall).
AUC-ROC (Area Under Curve): Площадь под ROC-кривой. Показывает, насколько модель способна отделять классы. Значение от 0.5 (случайный угадыватель) до 1.0 (идеальная модель).
5. Регуляризация: борьба с переобучением
Проблема: Модель может стать слишком сложной и "запомнить" обучающие данные (шум), вместо того чтобы изучить общие закономерности (переобучение, overfitting). Проявляется в виде огромных весов.
Решение: Добавить в функцию потерь штраф за большие веса.
L1-регуляризация (Lasso): Штрафует за сумму модулей весов. Может обнулять веса неважных признаков (отбор признаков).
L2-регуляризация (Ridge): Штрафует за сумму квадратов весов. Распределяет вес между всеми признаками, уменьшая их величину.
ElasticNet: Комбинация L1 и L2.
Вывод по теме 14: Линейная и логистическая регрессия — это фундаментальные, интерпретируемые и мощные алгоритмы. Линейная регрессия предсказывает числа, логистическая — вероятности классов. Ключ к их успешному применению — правильная подготовка данных (масштабирование для линейной регрессии), понимание метрик оценки и применение регуляризации для борьбы с переобучением.

Тема 15: Методы кластеризации данных
1. Суть кластеризации
Задача без учителя: Разделить данные на группы (кластеры) так, чтобы:
Объекты внутри одного кластера были максимально похожи (высокая внутрикластерная схожесть).
Объекты из разных кластеров были максимально различны (низкая межкластерная схожесть).
Важно: В кластеризации нет "правильных" ответов. Это инструмент для исследования, и результат зависит от выбора алгоритма, метрики и параметров.
2. Алгоритм K-Means (K-средних)
Идея: Задать число кластеров K. Алгоритм итеративно находит K центроидов (точек — центров кластеров) и присваивает каждому объекту кластер ближайшего к нему центроида.
Шаги алгоритма:
Инициализация: Случайный выбор K точек в качестве начальных центроидов.
Назначение: Каждому объекту назначается кластер, центроид которого ему ближе всего (по евклидову расстоянию).
Обновление: Для каждого кластера пересчитывается центроид как среднее арифметическое всех объектов этого кластера.
Повторение шагов 2 и 3 до сходимости (когда центроиды перестают значительно меняться или меняются назначения).
Критерий качества (инерция, inertia): Сумма квадратов расстояний от каждого объекта до центроида его кластера. Алгоритм стремится минимизировать инерцию.
3. Выбор числа кластеров (K)
Метод локтя (Elbow Method): Построение графика зависимости инерции от числа кластеров K. Ищем "локоть" — точку, после которой инерция падает не так резко. Это значение K часто берут за оптимальное.
Субъективная интерпретация: Лучшее K — то, которое дает осмысленные с точки зрения предметной области кластеры.
4. Особенности и проблемы K-Means
Требует указания K (числа кластеров) заранее.
Чувствительность к инициализации (случайному выбору начальных центроидов). Решение: n_init параметр (запуск алгоритма несколько раз с разной инициализацией и выбор лучшего результата).
Чувствительность к выбросам, так как центроид — это среднее.
Предполагает, что кластеры имеют сферическую форму и примерно одинаковый размер (из-за использования евклидова расстояния). Не справится с кластерами сложной формы.
Требует масштабирования признаков, так как использует евклидово расстояние.
5. Иерархическая кластеризация
Идея: Построить дерево кластеров (дендрограмма), где в корне — один большой кластер (все данные), а в листьях — отдельные объекты.
Подходы:
Агломеративная (снизу вверх): Начинаем с отдельных объектов, последовательно объединяем самые близкие кластеры.
Дивизимная (сверху в down): Начинаем с одного кластера, последовательно разделяем.
Способы определения расстояния между кластерами (linkage):
Single linkage: Расстояние между ближайшими точками двух кластеров. Склонен создавать длинные "цепочки".
Complete linkage: Расстояние между самыми дальними точками. Склонен создавать компактные кластеры.
Average linkage: Среднее расстояние между всеми парами точек из двух кластеров.
Результат: Дендрограмма. Выбор числа кластеров осуществляется "разрезанием" дендрограммы на нужной высоте.
6. Оценка качества кластеризации
Внутренние метрики (не требуют истинных меток):
Silhouette Score (силуэтный коэффициент): Оценивает, насколько объект похож на свой кластер по сравнению с другими кластерами. Диапазон от -1 до 1. Чем выше, тем лучше.
Calinski-Harabasz Index (Variance Ratio Criterion): Отношение межклассовой дисперсии к внутриклассовой. Чем выше, тем лучше разделены кластеры.
Внешние метрики (если истинные метки известны — для тестирования):
Adjusted Rand Index (ARI), Adjusted Mutual Information (AMI): Измеряют сходство между истинной разметкой и предсказанной кластеризацией, с поправкой на случайность.
Вывод по теме 15: Кластеризация — мощный инструмент для исследования структуры данных. K-Means — простой и популярный, но имеет ряд ограничений. Иерархическая кластеризация дает более наглядное представление (дендрограмма), но может быть медленной на больших данных. Выбор метода и параметров должен быть осмысленным и опираться на визуализацию и метрики.

Тема 16: Снижение размерности данных: PCA и другие методы
1. Зачем нужно снижение размерности?
Проклятие размерности (Curse of Dimensionality): С ростом числа признаков данные становятся "разреженными", расстояния между точками перестают быть информативными, сложность моделей растет.
Цели:
Визуализация: Проецируем данные на 2D/3D, чтобы увидеть структуру.
Ускорение алгоритмов (уменьшение объема вычислений).
Улучшение качества моделей: Удаление шума и избыточных признаков.
Сжатие данных.
2. Метод главных компонент (Principal Component Analysis, PCA)
Идея: Найти новые оси (главные компоненты), такие что:
Первая главная компонента (PC1) объясняет максимальную дисперсию в данных.
Вторая главная компонента (PC2) ортогональна первой и объясняет максимальную дисперсию из оставшейся, и так далее.
Математика: Линейное преобразование. Новые оси — это собственные векторы ковариационной матрицы исходных данных, отсортированные по убыванию собственных значений (которые показывают долю объясненной дисперсии).
Свойства главных компонент:
Ортогональны (некоррелированы).
Их количество равно min(n_samples, n_features).
Они являются линейными комбинациями исходных признаков.
3. Этапы применения PCA
Масштабирование данных (обязательно!): PCA чувствителен к масштабу. Если признаки в разных единицах, нужно применить StandardScaler.
Вычисление ковариационной матрицы (или SVD разложения).
Вычисление собственных векторов и значений.
Сортировка собственных векторов по убыванию собственных значений.
Выбор числа компонент k:
По доле объясненной дисперсии (Explained Variance Ratio): Обычно выбирают k, чтобы сохранить, например, 95% дисперсии.
По графику "каменистой осыпи" (Scree Plot): График собственных значений. Ищем "излом", после которого значения падают медленно.
Проецирование данных на выбранные k главных компонент: X_reduced = X_scaled @ W_k, где W_k — матрица из первых k собственных векторов.
4. Интерпретируемость PCA
Плюс: Новые признаки (PC) некоррелированы, что полезно для некоторых моделей.
Минус: Главные компоненты — это абстрактные линейные комбинации исходных признаков. Их сложно содержательно интерпретировать (например, что такое PC1 = 0.5*рост - 0.1*вес + 0.8*возраст?).
График нагрузки (Loading Plot): Показывает вклад каждого исходного признака в главные компоненты. Помогает понять, из чего "состоят" PC.
5. t-SNE (t-distributed Stochastic Neighbor Embedding) — для визуализации
Идея: Нелинейный метод, который фокусируется на сохранении локальных расстояний (похожие объекты в исходном пространстве должны быть близки в новом).
Особенности:
Отлично подходит для визуализации кластеров в 2D/3D.
Результаты запусков могут отличаться (стохастический алгоритм).
Медленный на больших данных.
Не сохраняет глобальную структуру (расстояния между кластерами на графике t-SNE могут ничего не значить).
Нельзя использовать для feature extraction в моделях, только для визуализации.
Применение: После того, как PCA сократил размерность до ~50, можно применить t-SNE для визуализации в 2D.
6. Другие методы
LDA (Linear Discriminant Analysis): Метод с учителем. Ищет оси, которые максимизируют разделение между классами (в отличие от PCA, который максимизирует дисперсию). Используется как для снижения размерности, так и для классификации.
UMAP (Uniform Manifold Approximation and Projection): Современный нелинейный метод, конкурент t-SNE. Часто лучше сохраняет глобальную структуру и работает быстрее.
Вывод по теме 16: Снижение размерности — критически важный этап для работы с высокомерными данными. PCA — основной линейный метод для сокращения числа признаков с минимальной потерей информации, но его компоненты сложно интерпретировать. t-SNE — прекрасный инструмент для визуализации сложных структур и кластеров в 2D. Выбор метода зависит от цели: ускорение моделей (PCA) или исследовательская визуализация (t-SNE/UMAP).



